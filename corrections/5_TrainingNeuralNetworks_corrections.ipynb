{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Notebook 5 - Optimization and neural networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.optimize\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "In this session we will talk about optimization in general and its application to machine learning.\n",
        "\n",
        "First we will look into a general setting. Let us simply minimize the function :\n",
        " $ f(x) = x^2 $ when starting from $x_0=2$\n",
        "\n",
        " A one-liner for that is to use scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "# Définir la fonction f(x) qui retourne le carré de x\n",
        "def f(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Définir une valeur initiale pour commencer l'optimisation\n",
        "x_0 = 2\n",
        "\n",
        "# Utiliser la fonction 'minimize' pour trouver la valeur de x qui minimise f(x)\n",
        "# La fonction commence à x_0 et cherche le minimum\n",
        "result = scipy.optimize.minimize(f, x_0)\n",
        "\n",
        "# Extraire la valeur de x qui minimise la fonction\n",
        "# Cela devrait être proche de zéro pour cette fonction\n",
        "result.x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "### Implementing a random search\n",
        "\n",
        "A first possible algorithm is to sample a change for x and keep the best value.\n",
        "We iterate the following steps :\n",
        "- take a neighbor for x, sampling a random number with standard variation 0.01.\n",
        "- evaluate these two possibilities\n",
        "- move to the best one\n",
        "\n",
        "Implement that with a for loop with 1000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "# Définir le nombre d'itérations pour l'algorithme d'optimisation\n",
        "n_iter = 1000\n",
        "\n",
        "# Initialiser x avec la valeur initiale x_0\n",
        "x = x_0\n",
        "\n",
        "# Créer une liste pour stocker tous les résultats de la fonction au cours des itérations\n",
        "all_results = list()\n",
        "\n",
        "# Définir une fonction qui échantillonne autour de la valeur actuelle de x\n",
        "# Elle ajoute un bruit gaussien avec un écart-type de 0.01\n",
        "def sample_around(x):\n",
        "    return x + np.random.normal(scale=0.01)\n",
        "\n",
        "# Boucle sur le nombre d'itérations spécifié\n",
        "for _ in range(n_iter):\n",
        "    # Échantillonner autour de la valeur actuelle de x\n",
        "    sample = sample_around(x)\n",
        "\n",
        "    # Calculer les valeurs de la fonction pour x et l'échantillon\n",
        "    f_x, f_sample = f(x), f(sample)\n",
        "\n",
        "    # Si la valeur de la fonction pour l'échantillon est inférieure à celle de x\n",
        "    # alors mettre à jour x avec la valeur de l'échantillon\n",
        "    if f_sample < f_x:\n",
        "        x = sample\n",
        "        all_results.append(f_sample)\n",
        "    # Sinon, conserver la valeur actuelle de x\n",
        "    else:\n",
        "        x = x\n",
        "        all_results.append(f_x)\n",
        "\n",
        "# Afficher la valeur finale de x après toutes les itérations\n",
        "print(x)\n",
        "\n",
        "# Tracer les valeurs de la fonction au cours des itérations\n",
        "plt.plot(all_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "### Implementing an exaustive search\n",
        "\n",
        "A first possible algorithm is to try all changes for x and keep the best value.\n",
        "We iterate the following steps :\n",
        "- try a smaller and a larger x value of 0.01.\n",
        "- evaluate these two possibilities\n",
        "- move to the best one\n",
        "\n",
        "Implement that with a for loop with 1000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "n_iter = 1000\n",
        "x = x_0\n",
        "all_results = list()\n",
        "\n",
        "for _ in range(n_iter):\n",
        "    # Calculer deux nouvelles valeurs autour de x : une plus petite et une plus grande\n",
        "    smaller, larger = x - 0.01, x + 0.01\n",
        "\n",
        "    # Calculer les valeurs de la fonction pour ces deux nouvelles valeurs\n",
        "    f_small, f_large = f(smaller), f(larger)\n",
        "\n",
        "    # Si la valeur de la fonction pour la valeur plus petite est inférieure à celle de la valeur plus grande\n",
        "    # alors mettre à jour x avec la valeur plus petite\n",
        "    if f_small < f_large:\n",
        "        x = smaller\n",
        "        all_results.append(f_small)\n",
        "    # Sinon, mettre à jour x avec la valeur plus grande\n",
        "    else:\n",
        "        x = larger\n",
        "        all_results.append(f_large)\n",
        "\n",
        "\n",
        "print(x)\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "### Implementing a gradient descent 'by hand'\n",
        "Now let us implement the gradient descent, by remembering that $\\frac{df}{dx} = 2x$\n",
        "\n",
        "We iterate the following steps :\n",
        "- compute the gradient value at x\n",
        "- Update x : $x \\leftarrow x - 0.01 \\frac{df}{dx}$\n",
        "\n",
        "Implement that with a for loop with 1000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "# Définir la dérivée de la fonction f(x) = x^2, qui est df(x) = 2x\n",
        "def df(x):\n",
        "    return 2 * x\n",
        "\n",
        "\n",
        "all_results = list()\n",
        "n_iter = 1000\n",
        "x = x_0\n",
        "for _ in range(n_iter):\n",
        "    # Calculer la dérivée de la fonction à la position actuelle de x\n",
        "    dx = df(x)\n",
        "\n",
        "    # Mettre à jour x en utilisant la méthode de descente de gradient\n",
        "    # On soustrait un petit multiple de la dérivée pour se déplacer vers le minimum\n",
        "    x = x - 0.01 * dx\n",
        "\n",
        "    # Ajouter la valeur de la fonction à la position actuelle de x à la liste des résultats\n",
        "    all_results.append(f(x))\n",
        "\n",
        "print(x)\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "### Implementing a gradient descent with automatic differentiation (by hand)\n",
        "\n",
        "We want to use the same algorithm but without knowing the formula of differentiation.\n",
        "We instead want to rely on Pytorch\n",
        "\n",
        "Below is the implementation of the same method as before, with PyTorch.\n",
        "\n",
        "Can you confirm that we get the same results ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "all_results = list()\n",
        "n_iter = 1000\n",
        "\n",
        "# Initialiser x comme un tenseur PyTorch avec une valeur initiale de 2.0\n",
        "# L'argument requires_grad=True permet de calculer les gradients pour ce tenseur\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "for i in range(n_iter):\n",
        "\n",
        "    # Calculer la valeur de la fonction f(x) = x^2\n",
        "    f_x = x ** 2\n",
        "\n",
        "    # Calculer le gradient de f_x par rapport à x\n",
        "    f_x.backward()\n",
        "\n",
        "    # Mettre à jour x en utilisant la méthode de descente de gradient\n",
        "    # On soustrait un petit multiple du gradient pour se déplacer vers le minimum\n",
        "    x.data = x - 0.01 * x.grad.item()\n",
        "\n",
        "    # Réinitialiser le gradient à None pour éviter l'accumulation des gradients\n",
        "    x.grad = None\n",
        "\n",
        "    # Ajouter la valeur de la fonction à la position actuelle de x à la liste des résultats\n",
        "    all_results.append(f_x.data)\n",
        "\n",
        "print(x.item())\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "### Implementing a gradient descent with automatic differentiation (the proper way)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "all_results = list()\n",
        "n_iter = 1000\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Créer un optimiseur SGD (Stochastic Gradient Descent) avec un taux d'apprentissage de 0.01\n",
        "# Le paramètre momentum est défini à 0, donc il n'est pas utilisé ici\n",
        "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
        "\n",
        "for i in range(n_iter):\n",
        "    # Calculer la valeur de la fonction f(x) = x^2\n",
        "    f_x = f(x)\n",
        "\n",
        "    # Calculer le gradient de f_x par rapport à x\n",
        "    f_x.backward()\n",
        "\n",
        "    # Mettre à jour x en utilisant l'optimiseur SGD\n",
        "    opt.step()\n",
        "\n",
        "    # Réinitialiser les gradients à zéro pour éviter l'accumulation des gradients\n",
        "    opt.zero_grad()\n",
        "    all_results.append(f_x.data)\n",
        "\n",
        "print(x.item())\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "## Bigger input space\n",
        "\n",
        "Let us now look at a more complicated input space, the function takes as input five numbers and returns :\n",
        "$f_2(x_1, x_2, x_3, x_4, x_5) = (x_1 + x_2 + x_3 + x_4 + x_5)^2$\n",
        "\n",
        "Now it is more costly to find the right direction randomly. Try the random algorithm on this new function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "# Définir une fonction f_2 qui prend un vecteur x en entrée\n",
        "# et retourne le carré de la somme de ses éléments\n",
        "def f_2(x):\n",
        "    return (x[0] + x[1] + x[2] + x[3] + x[4]) ** 2\n",
        "\n",
        "\n",
        "new_x_0 = (1, 2, 3, 4, 5)\n",
        "f_2(new_x_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": [],
      "source": [
        "n_iter = 10000\n",
        "x = new_x_0\n",
        "all_results = list()\n",
        "\n",
        "# Définir une fonction qui échantillonne autour de la valeur actuelle de x\n",
        "# Elle ajoute un bruit gaussien avec un écart-type de 0.01 à chaque composante de x\n",
        "def sample_around(x):\n",
        "    return x + np.random.normal(size=5, scale=0.01)\n",
        "\n",
        "\n",
        "for _ in range(n_iter):\n",
        "     # Échantillonner autour de la valeur actuelle de x\n",
        "    sample = sample_around(x)\n",
        "\n",
        "    f_x, f_sample = f_2(x), f_2(sample)\n",
        "\n",
        "    # Si la valeur de la fonction pour l'échantillon est inférieure à celle de x\n",
        "    # alors mettre à jour x avec la valeur de l'échantillon\n",
        "    if f_sample < f_x:\n",
        "        x = sample\n",
        "        all_results.append(f_sample)\n",
        "    # Sinon, conserver la valeur actuelle de x\n",
        "    else:\n",
        "        x = x\n",
        "        all_results.append(f_x)\n",
        "\n",
        "print(x)\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "source": [
        "Now let us try the gradient approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "all_results = list()\n",
        "n_iter = 1000\n",
        "x = torch.tensor(new_x_0, requires_grad=True, dtype=float)\n",
        "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
        "\n",
        "for i in range(n_iter):\n",
        "    f_x = f_2(x)\n",
        "    f_x.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    all_results.append(f_x.data)\n",
        "\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "source": [
        "## Actual machine learning examples\n",
        "\n",
        "\n",
        "Now instead of minimizing random functions, let us minimize the error of a linear model !\n",
        "\n",
        "We will use generated data (that I used during my class) : we simulate a hidden relationship (base_function) by sampling input-output pairs with noise.\n",
        "\n",
        "Let us generate the data once again and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Définir une graine pour le générateur de nombres aléatoires afin de garantir la reproductibilité\n",
        "np.random.seed(42)\n",
        "\n",
        "# Définir la fonction de base que nous allons échantillonner\n",
        "def base_function(x):\n",
        "    y = 1.3 * x ** 3 - 3 * x ** 2 + 3.6 * x + 6.9\n",
        "    return y\n",
        "\n",
        "# Définir les bornes inférieure et supérieure pour les valeurs de x\n",
        "low, high = -1, 3\n",
        "\n",
        "# Définir le nombre de points à échantillonner\n",
        "n_points = 80\n",
        "\n",
        "# Générer des valeurs aléatoires uniformément distribuées entre 'low' et 'high'\n",
        "# Chaque valeur est mise sous forme de tableau 2D avec une seule colonne\n",
        "xs = np.random.uniform(low, high, n_points)[:, None]\n",
        "\n",
        "# Calculer les valeurs de la fonction de base pour les points échantillonnés\n",
        "sample_ys = base_function(xs)\n",
        "\n",
        "# Ajouter du bruit gaussien aux valeurs échantillonnées\n",
        "ys_noise = np.random.normal(size=(len(xs), 1))\n",
        "noisy_sample_ys = sample_ys + ys_noise\n",
        "\n",
        "# Créer une série de points linéairement espacés entre 'low' et 'high'\n",
        "# Chaque point est mis sous forme de tableau 2D avec une seule colonne\n",
        "lsp = np.linspace(low, high)[:, None]\n",
        "\n",
        "# Calculer les valeurs de la fonction de base pour ces points linéairement espacés\n",
        "true_ys = base_function(lsp)\n",
        "\n",
        "# Tracer la fonction de base en pointillés\n",
        "plt.plot(lsp, true_ys, linestyle='dashed')\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "\n",
        "# Ajouter des étiquettes aux axes\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "\n",
        "# Afficher le graphique\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "source": [
        "### Gradient descent using torch.\n",
        "First create a torch version of these objects.\n",
        "\n",
        "We specify a float32 dtype for our objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "# Convertir le tableau NumPy 'noisy_sample_ys', 'xs' et 'lsp' en un tenseur PyTorch de type float\n",
        "# Cela permet d'utiliser les fonctionnalités de PyTorch pour les calculs ultérieurs\n",
        "\n",
        "torch_noisy_sample_ys = torch.from_numpy(noisy_sample_ys).float()\n",
        "torch_xs = torch.from_numpy(xs).float()\n",
        "torch_lsp = torch.from_numpy(lsp).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "source": [
        "\n",
        "Let us try to fit a linear model by hand, instead of simply relying on scikit-learn !\n",
        "\n",
        "The model of a linear regression is : $f_\\theta (x) = (\\theta_1 x + \\theta_0)$\n",
        "\n",
        "Careful ! We do not want to minimize the function of x itself.\n",
        "\n",
        "We want to minimise the errors we make, also called the loss function. We will do this by adjusting the parameters $\\theta$ of the function, starting from an arbitrary value of (1,1). This loss function is the sum of the square errors at each point :\n",
        "\n",
        "$$ \\min_{\\theta}\\mathcal{L} (\\theta) = 1/N\\sum_i (y_i - f_{\\theta} (x_i))^ 2 \\\\\n",
        "= 1/N\\sum_i (y_i - (\\theta_1 x_i + \\theta_0))^ 2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "outputs": [],
      "source": [
        "# Définir une fonction f_theta qui représente une droite d'équation y = theta[1] * x + theta[0]\n",
        "# Elle prend en entrée un tenseur x et un tenseur de paramètres theta\n",
        "def f_theta(x, theta):\n",
        "    return theta[1] * x + theta[0]\n",
        "\n",
        "# Définir une fonction de perte (loss function) qui calcule l'erreur quadratique moyenne\n",
        "# entre les valeurs prédites par f_theta et les valeurs bruitées (torch_noisy_sample_ys)\n",
        "def loss_function(theta):\n",
        "    return torch.mean((torch_noisy_sample_ys - f_theta(torch_xs, theta)) ** 2)\n",
        "\n",
        "# Initialiser les paramètres theta avec des valeurs initiales (1.0, 1.0)\n",
        "# requires_grad=True permet de calculer les gradients pour ces paramètres\n",
        "initial_theta = torch.tensor((1., 1.), requires_grad=True)\n",
        "\n",
        "# Calculer la valeur initiale de la fonction de perte avec les paramètres initiaux\n",
        "initial_loss = loss_function(initial_theta)\n",
        "print(initial_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "outputs": [],
      "source": [
        "all_results = list()\n",
        "n_iter = 1000\n",
        "\n",
        "theta = copy.deepcopy(initial_theta)\n",
        "opt = torch.optim.SGD([theta], lr=0.01, momentum=0.0)\n",
        "\n",
        "for i in range(n_iter):\n",
        "    loss_value = loss_function(theta)\n",
        "    loss_value.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    all_results.append(loss_value.data)\n",
        "\n",
        "print(theta.data)\n",
        "plt.plot(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "We have values for the parameters now.\n",
        "Let us look at what they look like.\n",
        "\n",
        "Use the theta_function on the linspace to plot your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "# Calculer les valeurs prédites par le modèle linéaire f_theta pour les points linéairement espacés (torch_lsp)\n",
        "# .detach() est utilisé pour détacher le tenseur du graphe de calcul, ce qui signifie que les opérations suivantes\n",
        "# ne seront pas suivies pour le calcul des gradients\n",
        "# .numpy() convertit le tenseur PyTorch en un tableau NumPy pour le tracé\n",
        "predicted_ys = f_theta(torch_lsp, theta).detach().numpy()\n",
        "\n",
        "# Tracer la fonction de base originale en pointillés\n",
        "plt.plot(lsp, true_ys, linestyle='dashed')\n",
        "\n",
        "# Tracer les valeurs prédites par le modèle linéaire\n",
        "plt.plot(lsp, predicted_ys)\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "# Initialiser les paramètres theta avec des valeurs initiales (1.0, 1.0)\n",
        "# requires_grad=True permet de calculer les gradients pour ces paramètres\n",
        "theta_0 = torch.tensor((1., 1.), requires_grad=True)\n",
        "\n",
        "n_iter = 30\n",
        "opt = torch.optim.SGD([theta_0], lr=0.02, momentum=0.0)\n",
        "\n",
        "for i in range(n_iter):\n",
        "    # Tous les 5 itérations, tracer les valeurs prédites par le modèle linéaire\n",
        "    # on utilise 'i' on n'utilise pas '_'\n",
        "    if i % 5 == 0:\n",
        "        predicted_ys = f_theta(torch_lsp, theta_0).detach().numpy()\n",
        "        plt.plot(lsp, predicted_ys, label='Iteration {}'.format(i))\n",
        "\n",
        "    loss_value = loss_function(theta_0)\n",
        "    loss_value.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "# Sauvegarder le graphique sous forme d'image avec une haute résolution\n",
        "plt.savefig(\"iterations.png\", dpi=600)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "## Deep Learning with PyTorch\n",
        "\n",
        "We start by training a small MLP using built-in functionalities in scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Créer une instance de MLPRegressor, un modèle de réseau de neurones pour la régression\n",
        "# max_iter=5000 spécifie le nombre maximum d'itérations pour l'entraînement\n",
        "mlp_model = MLPRegressor(max_iter=5000)\n",
        "\n",
        "# Entraîner le modèle MLP sur les données (xs, noisy_sample_ys)\n",
        "# xs sont les caractéristiques (features) et noisy_sample_ys sont les valeurs cibles (targets)\n",
        "# .flatten() est utilisé pour aplatir le tableau noisy_sample_ys en un vecteur 1D\n",
        "mlp_model.fit(xs, noisy_sample_ys.flatten())\n",
        "\n",
        "# Utiliser le modèle entraîné pour prédire les valeurs correspondant aux points linéairement espacés (lsp)\n",
        "predicted_lsp = mlp_model.predict(lsp)\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "\n",
        "# Tracer les prédictions du modèle MLP en orange avec une épaisseur de ligne de 2\n",
        "plt.plot(lsp, predicted_lsp, color='orange', lw=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "MLPRegressor works well for this simple data, but it lacks the more advanced deep learning modeling that PyTorch can offer.\n",
        "Let's start by achieving a similar result to MLPRegressor, but defining our model ourselves and in PyTorch.\n",
        "\n",
        "By default, the MLP Regressor makes the following computational graph :\n",
        "- input gets multiplied by a matrix with 100 parameters, and an additional parameter is added to each values, giving 100 outputs by (shape = (n_samples, 100))\n",
        "- ReLU is applied to each of these outputs (shape = (n_samples, 100)). The relu function is implemented in PyTorch with torch.nn.functional.relu(x)\n",
        "- Then this value is multiplied by a matrix to produce a scalar output (again 100 parameters) (shape = (n_samples, 1)) and shifted by an offset.\n",
        "\n",
        "A quick reminder on matrix multiplication : it is an operation that combines one matrix A of shape (m,n) and a matrix B of shape (n,p) into a matrix C of shape (m,p). In PyTorch (and NumPy), you need to call torch.matmul(A,B) to make this computation.\n",
        "\n",
        "To make the two big multiplications, we will use one torch tensor of 100 parameters for each multiplication, with the appropriate shape. Create random starting tensors of parameters.\n",
        "\n",
        "Then implement the asked computation to produce our output from our input. You should debug the operations by ensuring the shapes are correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "# Créer les paramètres du réseau avec des valeurs initiales aléatoires tirées d'une distribution normale\n",
        "# Ces paramètres sont les poids (w1, w2) et les biais (b1, b2) du réseau de neurones\n",
        "# requires_grad=True permet de calculer les gradients pour ces paramètres lors de l'optimisation\n",
        "\n",
        "# Créer le premier ensemble de poids w1 avec une moyenne de 0 et un écart-type de 0.1\n",
        "# La taille est (1, 100), ce qui signifie 1 ligne et 100 colonnes\n",
        "w1 = torch.normal(mean=0., std=0.1, size=(1, 100), requires_grad=True)\n",
        "\n",
        "# Créer le premier ensemble de biais b1 avec une moyenne de 0 et un écart-type de 0.1\n",
        "# La taille est (1, 100), correspondant aux biais pour chaque neurone de la première couche\n",
        "b1 = torch.normal(mean=0., std=0.1, size=(1, 100), requires_grad=True)\n",
        "\n",
        "# Créer le deuxième ensemble de poids w2 avec une moyenne de 0 et un écart-type de 0.1\n",
        "# La taille est (100, 1), ce qui signifie 100 lignes et 1 colonne\n",
        "w2 = torch.normal(mean=0., std=0.1, size=(100, 1), requires_grad=True)\n",
        "\n",
        "# Créer le deuxième ensemble de biais b2 avec une moyenne de 0 et un écart-type de 0.1\n",
        "# La taille est (1,), correspondant au biais pour le neurone de sortie\n",
        "b2 = torch.normal(mean=0., std=0.1, size=(1,), requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "# Définir la fonction f qui représente le réseau de neurones\n",
        "# Elle prend en entrée un tenseur x et utilise les poids et biais définis précédemment\n",
        "def f(x, weight1=w1, bias1=b1, weight2=w2, bias2=b2):\n",
        "    # Calculer la sortie de la première couche en effectuant une multiplication matricielle\n",
        "    # entre l'entrée x et les poids weight1, puis ajouter le biais bias1\n",
        "    y1 = torch.matmul(x, weight1) + bias1\n",
        "\n",
        "    # Appliquer la fonction d'activation ReLU à la sortie de la première couche\n",
        "    a1 = torch.nn.functional.relu(y1)\n",
        "\n",
        "    # Calculer la sortie finale en effectuant une multiplication matricielle\n",
        "    # entre la sortie activée a1 et les poids weight2, puis ajouter le biais bias2\n",
        "    out = torch.matmul(a1, weight2) + bias2\n",
        "\n",
        "    # Retourner la sortie finale du réseau\n",
        "    return out\n",
        "\n",
        "# Vérifier que lors de l'inférence sur les données, nous obtenons un tenseur de sortie de forme (80, 1)\n",
        "# Cela correspond à 80 prédictions, une pour chaque échantillon dans torch_xs\n",
        "f(torch_xs).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "source": [
        "Now we will mostly use the optimization procedure above to train our network using Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": [],
      "source": [
        "# Définir le nombre d'itérations pour l'algorithme d'optimisation\n",
        "n_iter = 2000\n",
        "# L'optimiseur prend en entrée une liste contenant tous les paramètres du réseau : w1, b1, w2, b2\n",
        "opt = torch.optim.SGD([w1, b1, w2, b2], lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "outputs": [],
      "source": [
        "# Boucle sur le nombre d'itérations spécifié pour l'entraînement du réseau\n",
        "for i in range(n_iter):\n",
        "    # Effectuer une passe avant (forward pass) : calculer les prédictions du réseau pour les données d'entrée\n",
        "    prediction = f(torch_xs, w1, b1, w2, b2)\n",
        "\n",
        "    # Calculer la perte en utilisant l'erreur quadratique moyenne entre les prédictions et les valeurs cibles bruitées\n",
        "    loss = torch.mean((prediction - torch_noisy_sample_ys) ** 2)\n",
        "\n",
        "    # Effectuer une passe arrière (backward pass) : calculer les gradients de la perte par rapport aux paramètres\n",
        "    loss.backward()\n",
        "\n",
        "    # Mettre à jour les paramètres du réseau en utilisant l'optimiseur SGD\n",
        "    opt.step()\n",
        "\n",
        "    # Réinitialiser les gradients à zéro pour éviter l'accumulation des gradients des itérations précédentes\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Tous les 100 itérations, afficher le numéro de l'itération et la valeur actuelle de la perte\n",
        "    if not i % 100:\n",
        "        print(i, loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "# Calculer les valeurs prédites par le modèle de réseau de neurones pour les points linéairement espacés (torch_lsp)\n",
        "# .detach() est utilisé pour détacher le tenseur du graphe de calcul, ce qui signifie que les opérations suivantes\n",
        "# ne seront pas suivies pour le calcul des gradients\n",
        "# .numpy() convertit le tenseur PyTorch en un tableau NumPy pour le tracé\n",
        "predicted_ys = f(torch_lsp).detach().numpy()\n",
        "\n",
        "# Tracer la fonction de base originale en pointillés\n",
        "plt.plot(lsp, true_ys, linestyle='dashed')\n",
        "\n",
        "# Tracer les valeurs prédites par le modèle de réseau de neurones\n",
        "plt.plot(lsp, predicted_ys)\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "Congratulations, you have coded yourself a MLP model ! We have used the computation graph framework.\n",
        "\n",
        "\n",
        "Now let us make our code prettier (more Pytorch) and more efficient.\n",
        "First let us refactor the model in the proper way it should be coded, by using the torch.nn.Module class.\n",
        "You should add almost no new code, just reorganize the one above into a class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module, Parameter\n",
        "\n",
        "# Définir une classe MyOwnMLP qui hérite de la classe Module de PyTorch\n",
        "class MyOwnMLP(Module):\n",
        "\n",
        "    # Initialiser les paramètres du réseau de neurones dans le constructeur\n",
        "    def __init__(self):\n",
        "        # Appeler le constructeur de la classe parente (Module)\n",
        "        super(MyOwnMLP, self).__init__()\n",
        "\n",
        "        # Définir les poids et biais de la première couche comme des paramètres de la classe\n",
        "        # Utiliser une distribution normale pour initialiser les valeurs\n",
        "        self.w1 = Parameter(torch.normal(mean=0., std=0.1, size=(1, 100)))\n",
        "        self.b1 = Parameter(torch.normal(mean=0., std=0.1, size=(1, 100)))\n",
        "\n",
        "        # Définir les poids et biais de la deuxième couche comme des paramètres de la classe\n",
        "        self.w2 = Parameter(torch.normal(mean=0., std=0.1, size=(100, 1)))\n",
        "        self.b2 = Parameter(torch.normal(mean=0., std=0.1, size=(1,)))\n",
        "\n",
        "    # Définir la méthode forward qui spécifie le passage avant du réseau\n",
        "    def forward(self, x):\n",
        "        # Calculer la sortie de la première couche\n",
        "        y1 = torch.matmul(x, self.w1) + self.b1\n",
        "\n",
        "        # Appliquer la fonction d'activation ReLU à la sortie de la première couche\n",
        "        a1 = torch.nn.functional.relu(y1)\n",
        "\n",
        "        # Calculer la sortie finale du réseau\n",
        "        out = torch.matmul(a1, self.w2) + self.b2\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instancier le modèle MyOwnMLP\n",
        "model = MyOwnMLP()\n",
        "\n",
        "# Effectuer un passage avant avec les données d'entrée torch_xs\n",
        "out = model(torch_xs)\n",
        "\n",
        "# Vérifier la forme du tenseur de sortie\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "Now we are good to also make the data iteration process look like Pytorch code !\n",
        "\n",
        "We need to define a Dataset object. Once we have this, we can use it to create a DataLoader object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_x, data_y):\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data_x[idx]\n",
        "        y = self.data_y[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": [],
      "source": [
        "# Créer une instance de CustomDataset avec les données d'entrée torch_xs et les étiquettes torch_noisy_sample_ys\n",
        "dataset = CustomDataset(data_x=torch_xs, data_y=torch_noisy_sample_ys)\n",
        "\n",
        "# Créer un DataLoader pour le dataset\n",
        "# batch_size=10 : le DataLoader fournira des lots (batches) de 10 échantillons à la fois\n",
        "# num_workers=6 : utiliser 6 processus pour charger les données en parallèle, ce qui peut accélérer le processus\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=10, num_workers=6)\n",
        "\n",
        "# Enregistrer le temps de début pour mesurer la durée de l'opération\n",
        "start = time.time()\n",
        "\n",
        "# Boucle sur chaque lot de données fourni par le DataLoader\n",
        "for point in dataloader:\n",
        "    # On ne fait rien avec les données, on passe simplement à l'itération suivante\n",
        "    pass\n",
        "\n",
        "# Calculer et afficher le temps écoulé pour parcourir tous les lots de données\n",
        "print('Done in pytorch : ', time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "source": [
        "The last thing missing to make our pipeline truly Pytorch is to use a GPU.\n",
        "\n",
        "In Pytorch it is really easy, you just need to 'move' your tensors to a 'device'.\n",
        "You can test if a gpu is available and create the appropriate device with the following lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "torch_xs = torch_xs.to(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "Now we finally have all the elements to make an actual Pytorch complete pipeline !\n",
        "\n",
        "Create a model, and try to put it on a device.\n",
        "Create an optimizer with your model's parameters\n",
        "Make your data into a dataloader\n",
        "\n",
        "Then use two nested for loops : one for 100 epochs, and in each epoch loop over the dataloader\n",
        "    Inside the loop, for every batch first put the data on the device\n",
        "    Then use the semantics of above :\n",
        "        - model(batch)\n",
        "        - loss computation and backward\n",
        "        - gradient step and zero_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "model = MyOwnMLP()\n",
        "\n",
        "# Créer un optimiseur Adam pour ajuster les paramètres du modèle\n",
        "# Adam est un algorithme d'optimisation qui adapte le taux d'apprentissage pour chaque paramètre\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Créer une instance de CustomDataset avec les données d'entrée torch_xs et les étiquettes torch_noisy_sample_ys\n",
        "dataset = CustomDataset(data_x=torch_xs, data_y=torch_noisy_sample_ys)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=10, num_workers=0)\n",
        "\n",
        "loss = 0\n",
        "# Boucle sur le nombre d'époques spécifié pour l'entraînement\n",
        "for epoch in range(n_epochs):\n",
        "    # Boucle sur chaque lot de données fourni par le DataLoader\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        # Transférer les données du lot sur le périphérique spécifié (GPU ou CPU)\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # Effectuer une passe avant : calculer les prédictions du modèle pour le lot d'entrée\n",
        "        prediction = model(batch_x)\n",
        "\n",
        "        # Calculer la perte en utilisant l'erreur quadratique moyenne entre les prédictions et les valeurs cibles\n",
        "        loss = torch.mean((prediction - batch_y) ** 2)\n",
        "\n",
        "        # Effectuer une passe arrière : calculer les gradients de la perte par rapport aux paramètres\n",
        "        loss.backward()\n",
        "\n",
        "        # Mettre à jour les paramètres du modèle en utilisant l'optimiseur Adam\n",
        "        opt.step()\n",
        "\n",
        "        # Réinitialiser les gradients à zéro pour éviter l'accumulation des gradients des itérations précédentes\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Convertir la perte en une valeur scalaire\n",
        "        loss = loss.item()\n",
        "    # Tous les 10 époques, afficher le numéro de l'époque et la valeur actuelle de la perte\n",
        "    if not epoch % 10:\n",
        "        print(epoch, loss)\n",
        "\n",
        "# Transférer le modèle entraîné sur le CPU pour une utilisation ultérieure\n",
        "model = model.to('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "Finally, we can plot the last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "plt.plot(lsp, true_ys, linestyle='dashed')\n",
        "\n",
        "# Tracer les valeurs prédites par le modèle de réseau de neurones\n",
        "plt.plot(lsp, predicted_ys)\n",
        "\n",
        "# Tracer les échantillons bruitées\n",
        "plt.scatter(xs, noisy_sample_ys)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "This is the end of the practical part of training neural networks !\n",
        "\n",
        "Of course, a lot more can be done. On this simple toy data, you can try to illustrate concepts of this class:\n",
        "- What happens if you use only 10 data points and increase the noise level ?\n",
        "- Can you observe an overfitting behavior ?\n",
        "- Can you see the impact of using different optimisers (SGD vs Adam) ?\n",
        "- ...\n",
        "\n",
        "Another interesting extension is to use a more advanced (yet manageable dataset), such as FashionMnist.\n",
        "You can use it through the built-in PyTorch objects: _torchvision.datasets.FashionMNIST_ .\n",
        "You can install torchvision with _pip install torchvision_ .\n",
        "More generally, you can follow this tutorial: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html to access the data and have a first model example and training:\n",
        "- Can you compare MLP architectures with CNNs on this task ?\n",
        "- Do you see an overfit on this dataset ?\n",
        "- Does data augmentation helps training on this dataset ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "izMgLJeCWzDv"
      },
      "id": "izMgLJeCWzDv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05bfc61"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We have reached the end of this notebook. Here's a summary of what we've covered, with the key points:\n",
        "\n",
        "- We started by exploring general optimization techniques on simple functions like $f(x) = x^2$ and $f_2(x_1, ..., x_5) = (\\sum x_i)^2$. We implemented various methods, including random search, exhaustive search, manual gradient descent, and PyTorch's automatic differentiation.\n",
        "- We observed how gradient descent, especially with automatic differentiation, is a powerful and efficient method for finding minima, particularly as the input space becomes larger.\n",
        "- We then applied these optimization principles to a machine learning context by manually implementing a linear regression model. We learned about minimizing a loss function (mean squared error) by adjusting model parameters (theta) using PyTorch's automatic differentiation and an optimizer (`torch.optim.SGD`).\n",
        "- Moving to more complex models, we briefly used scikit-learn's `MLPRegressor` to fit a simple neural network.\n",
        "- Finally, we delved into building our own Multi-Layer Perceptron (MLP) from scratch using PyTorch's core functionalities. This involved:\n",
        "    - Defining the model architecture using `torch.nn.Module` and `torch.nn.Parameter` to manage weights and biases.\n",
        "    - Structuring our data with `torch.utils.data.Dataset` and efficiently loading it in batches with `torch.utils.data.DataLoader`.\n",
        "    - Implementing the full training loop, including forward pass, loss calculation, backward pass for gradient computation, and parameter updates using an optimizer (`torch.optim.Adam`).\n",
        "    - Setting up the environment to leverage GPU acceleration by moving tensors and the model to a `device` (cuda or cpu).\n",
        "\n",
        "This notebook illustrated the fundamental concepts behind optimization and how they are applied in machine learning, culminating in a complete PyTorch pipeline for training neural networks."
      ],
      "id": "d05bfc61"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}