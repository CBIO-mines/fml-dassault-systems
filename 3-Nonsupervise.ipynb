{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 3 : Apprentissage non-supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Notebook préparé par [Chloé-Agathe Azencott](http://cazencott.info).\n",
    "\n",
    "Dans ce notebook il s'agit d'explorer plusieurs techniques de réduction de dimension et de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger numpy as np, matplotlib as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # règle la taille de police globalement pour les plots (en pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons effectuer une analyse en composantes principales d'un jeu de données décrivant les scores obtenus par les meilleurs athlètes ayant participé en 2004 à une épreuve de décathlon, aux Jeux Olympiques d'Athènes ou au Décastar de Talence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Les données sont contenues dans le fichier `decathlon.txt`.\n",
    "\n",
    "Le fichier contient 42 lignes et 13 colonnes.\n",
    "\n",
    "La première ligne est un en-tête qui décrit les contenus des colonnes.\n",
    "\n",
    "Les lignes suivantes décrivent les 41 athlètes.\n",
    "\n",
    "Les 10 premières colonnes contiennent les scores obtenus aux différentes épreuves.\n",
    "La 11ème colonne contient le classement.\n",
    "La 12ème colonne contient le nombre de points obtenus.\n",
    "La 13ème colonne contient une variable qualitative qui précise l'épreuve (JO ou Décastar) concernée.\n",
    "\n",
    "Nous allons examiner ces données avec la librairie `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('data/decathlon.txt', sep=\"\\t\")  # lire les données dans un dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "__Alternativement :__ Si vous avez besoin de télécharger le fichier (par exemple sur colab) :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11",
   "metadata": {},
   "source": [
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems/main/data/decathlon.txt\n",
    "\n",
    "my_data = pd.read_csv('decathlon.txt', sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Une __matrice de nuages de points__ est une visualisation en k x k panneaux des relations deux à deux entre k variables :\n",
    "* sur la diagonale, l'histogramme pour chacune des variables \n",
    "* hors diagonale, les nuages de points entre deux variables (non standardisées).\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(my_data[['Shot.put','High.jump', '400m']], alpha=0.5, s=60,\n",
    "               figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(my_data, alpha=0.5, s=60,\n",
    "               figsize=(18, 18));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Alternativement, la librairie `seaborn` permet des visualisations plus élaborées que `matplotlib`. Vous pouvez par exemple explorer les capacités de `jointplot`. \n",
    "https://seaborn.pydata.org/generated/seaborn.jointplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              height=6, space=0, color='b')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              kind='reg', height=6, space=0, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Nous allons maintenant effectuer une analyse en composantes principales des scores aux 10 épreuves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Commençons par extraire les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(my_data.drop(columns=['Points', 'Rank', 'Competition']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Standardisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Calcul des composantes principales\n",
    "\n",
    "Les algorithmes de factorisation de matrice de `scikit-learn` sont inclus dans le module `decomposition`. Pour  l'ACP, référez-vous à : \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Remarque : Nous avons ici peu de variables et pouvons nous permettre de calculer toutes les PC. \n",
    "\n",
    "La plupart des algorithmes implémentés dans `scikit-learn` suivent le fonctionnement suivant : \n",
    "* on instancie un objet, correspondant à un type d'algorithme/modèle, avec ses hyperparamètres (ici le nombre de composantes)\n",
    "* on utilise la méthode `fit` pour passer les données à cet algorithme\n",
    "* les paramètres appris sont maintenant accessibles comme arguments de cet objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet PCA pour 10 composantes principales\n",
    "pca = decomposition.PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On passe maintenant les données standardisées à cet objet\n",
    "# C'est ici que se font les calculs\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Proportion de variance expliquée par les PCs\n",
    "\n",
    "Nous allons maintenant afficher la proportion de variance expliquée par les différentes composantes. Il est accessible dans le paramètre `explained_variance_ration_` de notre objet `pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.ylabel(\"Proportion de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Nous pouvons aussi afficher la proportion *cumulative* de variance expliquée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.title(\"Proportion cumulative de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "On peut aussi calculer la proportion de variance expliquée par les 4 (par exemple) premières PC avec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.2f\" % np.sum(pca.explained_variance_ratio_[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "__Questions :__ \n",
    "* Quelle est la proportion de variance expliquée par les deux premières composantes ? \n",
    "* Combien de composantes faudrait-il utiliser pour expliquer 80% de la variance des données ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.2f\" % np.sum(pca.explained_variance_ratio_[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Projection des données sur les deux premières composantes principales\n",
    "\n",
    "Nous allons maintenant utiliser uniquement les deux premières composantes principales.\n",
    "\n",
    "Commençons par calculer la nouvelle représentation des données, c'est-à-dire leur projection sur ces deux PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "On peut afficher un nuage de points représentant les données selon ces deux PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "On peut maintenant colorer chaque point du nuage de points ci-dessus en fonction du classement de l'athlète qu'il représente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=my_data['Rank'])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "__Question :__ Qu'en conclure sur l'interprétation de la PC1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Interprétation des deux composantes principales\n",
    "Chaque composante principale est une combinaison linéaire des variables décrivant les données. Les poids de cette combinaison linéaire sont accesibles dans `pca.components_`.\n",
    "\n",
    "Nous pouvons visualiser non pas les individus, mais les 10 variables dans l'espace des 2 composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "print(pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(pcs[0], pcs[1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(pcs[0], pcs[1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "__Question :__ Quelles variables ont des contributions très similaires aux deux composantes principales ? Qu'en déduire sur leur similarité ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "__Question :__ Comment interpréter le signe des contributions des variables à la première composantes principales ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## 2. Données « Olivetti »"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser la réduction de dimension pour représenter en deux dimensions un jeu de données contenant des visages. Il s'agit d'un jeu de données classique, contenant 400 photos de 64 par 64 pixels. Il s'agit de photos des visages de 40 personnes différentes (10 photos par personne), étiquetées par un numéro de classe entre 0 et 39 identifiant la personne.\n",
    "\n",
    "Nous pouvons charger ce jeu de données directement grâce à scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "__Si vous n'arrivez pas à télécharger les données :__\n",
    "* Aller sur : https://github.com/CroncLee/PCA-face-recognition/blob/master/olivetti_py3.pkz\n",
    "* Télécharger le fichier (bouton Download) \n",
    "utiliser la commande\n",
    "```\n",
    "    data = datasets.fetch_olivetti_faces(data_home=\"<PATH TO DATA>\")\n",
    "```\n",
    "En remplaçant <PATH TO DATA> par le chemin vers le dossier où vous avez enregistré les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Les données contiennent %d classes\" % len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Chaque image est représentée par une valeur (niveau de gris) pour chacun de ses pixels. \n",
    "\n",
    "Nous pouvons visualiser ces images à condition de réorganiser ces valeurs (= un vecteur de longueur 4096) en matrices 64x64. Par exemle ci-dessous pour l'image à l'index 23 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[13, :].reshape((64, 64)), interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA\n",
    "\n",
    "Commençons par une analyse en composantes principales comme à la section précédente :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "X_transformed_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Chaque image est maintenant représentée par non pas 4096 variables, mais par deux variables. Nous pouvons les visualiser en nuage de point, et les colorer par classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_pca[:, 0], X_transformed_pca[:, 1], \n",
    "            c=y)\n",
    "plt.xlabel(\"Première composante principale\")\n",
    "plt.ylabel(\"Deuxième composante principale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "__Question :__ les images du même visage (= de la même classe) ont-elles des représentations proches ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Nous pouvons visualiser la contribution de chaque pixel à la première composante principale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[0, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Puis la contribution de chaque pixel à la deuxième composante principale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[1, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### tSNE\n",
    "\n",
    "Nous allons maintenant utiliser la même démarche, mais avec tSNE, grâce à la classe [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) du module `manifold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X_transformed = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], \n",
    "            c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### Influence du paramètre de perplexité "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate='auto', perplexity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X_transformed = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], \n",
    "            c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='random', learning_rate='auto', perplexity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X_transformed = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], \n",
    "            c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## 3. Cercles imbriqués"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "Générons un jeu de données en deux dimensions formé de deux cercles imbriqués :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de points\n",
    "n_samples = 1500\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(37)\n",
    "\n",
    "circles_X, circles_labels = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "Visualisons ces données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_X[:, 0], circles_X[:, 1], c=circles_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Supposons maintenant ne pas disposer des étiquettes. Quels algorithmes de clustering permettent de trouver deux clusters, correspondant chacun à un des cercles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### Algorithme des k-moyennes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "L'objectif de l'algorithme k-means est retrouver $K$ clusters (et leur centroïde $\\mu_k$) de manière à **minimiser la variance intra-cluster** :\n",
    "\n",
    "\\begin{align}\n",
    "V = \\sum_{k = 1}^{K} \\sum_{x \\in C_k} \\frac{1}{|C_k|} (\\|x - \\mu_k\\|^2)\n",
    "\\end{align}\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un k-means avec k=2\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "\n",
    "# application aux données \n",
    "kmeans.fit(circles_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "L'attribut `.labels_` contient, pour chaque observation, le numéro du cluster auquel cette observation est assignée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_X[:, 0], circles_X[:, 1], c=kmeans.labels_)\n",
    "plt.title(\"Clustering K-means (K=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "#### Trouver K avec le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "Le coefficient (ou score) de silhouette permet de **comparer les distances moyennes intra- et inter-cluster** :\n",
    "\n",
    "\\begin{align}\n",
    "\\text{score} = \\frac{b - a}{\\max(a, b)}\n",
    "\\end{align}\n",
    "\n",
    "avec $a$ la distance moyenne intra-cluster et $b$ la distance d'un point au cluster étranger le plus proche. Le score se calcule par observation (avec une valeur entre -1 et 1) puis la moyenne de ce score permet d'évaluer le clustering du nuage de point dans son ensemble.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient de silhouette pour le k-means (k=2) : %.2f\" % metrics.silhouette_score(circles_X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "k_values = range(2, 9)\n",
    "for kval in k_values:\n",
    "    kmeans_k = cluster.KMeans(n_clusters=kval)\n",
    "    kmeans_k.fit(circles_X)\n",
    "    silhouettes.append(metrics.silhouette_score(circles_X, kmeans_k.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, silhouettes)\n",
    "\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"silhouette\")\n",
    "\n",
    "print(\"Coefficient de silhouette du KMeans en fonction de K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = np.max(silhouettes)\n",
    "print(\"Coefficient de silhouette optimal : %.2f\" % best_silhouette)\n",
    "best_K = k_values[silhouettes.index(best_silhouette)]\n",
    "print(\"K correspondant : %.2f\" % best_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k = cluster.KMeans(n_clusters=best_K)\n",
    "kmeans_k.fit(circles_X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_X[:, 0], circles_X[:, 1], c=kmeans_k.labels_)\n",
    "plt.title(\"Clustering K-means (K=%d)\" % best_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN (Clustering par densité)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "L'algorithme DBSCAN (Density-Based Spatial Clustering of Applications with Noise) fonctionne en deux temps :\n",
    "- Toutes les observations suffisamment proches sont connectées entre elles.\n",
    "- Les observations avec un nombre minimal de voisins connectés sont considérées comme des *core samples*, à partir desquelles les clusters sont étendues. **Toutes les observations suffisamment proche d'un *core sample* appartiennent au même cluster que celui-ci**. \n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un clustering DBSCAN\n",
    "dbscan = cluster.DBSCAN(eps=0.2, min_samples=2)\n",
    "\n",
    "# application aux données \n",
    "dbscan.fit(circles_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "L'attribut `.labels_` contient, pour chaque observation, le numéro du cluster auquel cette observation est assignée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_X[:, 0], circles_X[:, 1], c=dbscan.labels_)\n",
    "plt.title(\"Clustering DBSCAN (eps=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "#### Rôle du paramètre de taille de voisinage (`eps`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "Si `eps` est trop petit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un clustering DBSCAN\n",
    "dbscan_005 = cluster.DBSCAN(eps=0.05, min_samples=2)\n",
    "\n",
    "# application aux données \n",
    "dbscan_005.fit(circles_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan_005.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "L'attribut `.labels_` contient, pour chaque observation, le numéro du cluster auquel cette observation est assignée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "outliers = np.where(dbscan_005.labels_ == -1)[0]\n",
    "plt.scatter(circles_X[outliers, 0], circles_X[outliers, 1], marker='*', color='red')\n",
    "\n",
    "non_outliers = np.where(dbscan_005.labels_ != -1)[0]\n",
    "plt.scatter(circles_X[non_outliers, 0], circles_X[non_outliers, 1], c=dbscan_005.labels_[non_outliers])\n",
    "plt.title(\"Clustering DBSCAN (eps=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "Si `eps` est trop grand :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un clustering DBSCAN\n",
    "dbscan_2 = cluster.DBSCAN(eps=2., min_samples=2)\n",
    "\n",
    "# application aux données \n",
    "dbscan_2.fit(circles_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan_2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles_X[:, 0], circles_X[:, 1], c=dbscan_2.labels_)\n",
    "plt.title(\"Clustering DBSCAN (eps=2.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Trouver eps avec le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient de silhouette pour DBSCAN (eps=0.2) : %.2f\" % metrics.silhouette_score(circles_X, dbscan.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.logspace(-3, 1, 40)\n",
    "silhouettes = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan_eps = cluster.DBSCAN(eps=eps, min_samples=2)\n",
    "    dbscan_eps.fit(circles_X)\n",
    "    if len(np.unique(dbscan_eps.labels_)) > 1: # nécessaire pour calculer le coeff de silhouette\n",
    "        silhouettes.append(metrics.silhouette_score(circles_X, dbscan_eps.labels_))\n",
    "    else:\n",
    "        silhouettes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_values, silhouettes)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eps (échelle log)\")\n",
    "plt.ylabel(\"silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = np.max(silhouettes)\n",
    "print(\"Coefficient de silhouette optimal : %.2f\" % best_silhouette)\n",
    "print(\"Eps correspondant : %.2f\" % eps_values[silhouettes.index(best_silhouette)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "### Index de Rand ajusté"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "L'index de Rand ajusté permet de **comparer un résultat de clustering avec des étiquettes**. Pour chaque paire d'observations nous regardons si elles se situent dans le même cluster ou non, dans le clustering prédit et réel. L'index prend des valeurs entre 0 (clustering aléatoire) et 1 (clustering parfait).\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "__Question :__ Pourquoi ne pas utiliser une métrique d'évaluation de modèle de classification ici ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté du K-means (K=2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté de dbscan (eps=0.2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, dbscan.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## 2. Manchots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "On reprend ici les données utilisées dans le notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins = pd.read_csv(\"data/penguins_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "__Alternativement :__ Si vous avez besoin de télécharger le fichier (par exemple sur colab) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems/main/data/penguins_data.csv\n",
    "\n",
    "palmerpenguins = pd.read_csv(\"penguins_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins = palmerpenguins[palmerpenguins['bill_depth_mm'].notna()]\n",
    "palmerpenguins = palmerpenguins.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X = np.array(palmerpenguins[[\"bill_length_mm\", \"body_mass_g\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation (centrer-réduire)\n",
    "penguins_X = preprocessing.StandardScaler().fit_transform(penguins_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names, species_int = np.unique(palmerpenguins.species, return_inverse=True)\n",
    "penguins_labels = species_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels)\n",
    "plt.xlabel(\"bill_length_mm (centrée-réduite)\")\n",
    "plt.ylabel(\"body_mass_g (centrée-réduite)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un k-means avec k=3\n",
    "kmeans = cluster.KMeans(n_clusters=3)\n",
    "\n",
    "# application aux données \n",
    "kmeans.fit(penguins_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=kmeans.labels_, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (centrée-réduite)\")\n",
    "plt.ylabel(\"body_mass_g (centrée-réduite)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient de silhouette pour le k-means (k=3) : %.2f\" % metrics.silhouette_score(penguins_X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté du K-means (K=3) : %.2f\" % metrics.adjusted_rand_score(penguins_labels, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.logspace(-3, 1, 40)\n",
    "silhouettes = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan_eps = cluster.DBSCAN(eps=eps, min_samples=2)\n",
    "    dbscan_eps.fit(penguins_X)\n",
    "    if len(np.unique(dbscan_eps.labels_)) > 1: # nécessaire pour calculer le coeff de silhouette\n",
    "        silhouettes.append(metrics.silhouette_score(penguins_X, dbscan_eps.labels_))\n",
    "    else:\n",
    "        silhouettes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_values, silhouettes)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eps (échelle log)\")\n",
    "plt.ylabel(\"silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = np.max(silhouettes)\n",
    "print(\"Coefficient de silhouette optimal : %.2f\" % best_silhouette)\n",
    "best_eps = eps_values[silhouettes.index(best_silhouette)]\n",
    "print(\"Eps correspondant : %.2f\" % best_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_opt = cluster.DBSCAN(eps=best_eps, min_samples=2)\n",
    "dbscan_opt.fit(penguins_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan_opt.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté de DBSCAN : %.2f\" % metrics.adjusted_rand_score(penguins_labels, dbscan_opt.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=dbscan_opt.labels_, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (centrée-réduite)\")\n",
    "plt.ylabel(\"body_mass_g (centrée-réduite)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "### Modèle de mélange gaussien "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "Le modèle de mélange de gaussiennes cherche à **optimiser les paramètres d'un nombre fini de gaussiennes** aux données. \n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation d'un k-means avec k=3\n",
    "gmm = mixture.GaussianMixture(n_components=3)\n",
    "\n",
    "# application aux données \n",
    "gmm.fit(penguins_X)\n",
    "\n",
    "# prédiction des clusters\n",
    "gmm_labels = gmm.predict(penguins_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=gmm_labels, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (centrée-réduite)\")\n",
    "plt.ylabel(\"body_mass_g (centrée-réduite)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient de silhouette pour le GMM (k=3) : %.2f\" % metrics.silhouette_score(penguins_X, gmm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté du GMM (K=3) : %.2f\" % metrics.adjusted_rand_score(penguins_labels, gmm_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
