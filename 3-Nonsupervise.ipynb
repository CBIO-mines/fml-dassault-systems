{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 3 : Apprentissage non-supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Notebook préparé par [Chloé-Agathe Azencott](http://cazencott.info).\n",
    "\n",
    "Dans ce notebook il s'agit d'explorer plusieurs techniques de réduction de dimension et de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger numpy as np, matplotlib as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # règle la taille de police globalement pour les plots (en pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons effectuer une analyse en composantes principales d'un jeu de données décrivant les scores obtenus par les meilleurs athlètes ayant participé en 2004 à une épreuve de décathlon, aux Jeux Olympiques d'Athènes ou au Décastar de Talence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Les données sont contenues dans le fichier `decathlon.txt`.\n",
    "\n",
    "Le fichier contient 42 lignes et 13 colonnes.\n",
    "\n",
    "La première ligne est un en-tête qui décrit les contenus des colonnes.\n",
    "\n",
    "Les lignes suivantes décrivent les 41 athlètes.\n",
    "\n",
    "Les 10 premières colonnes contiennent les scores obtenus aux différentes épreuves.\n",
    "La 11ème colonne contient le classement.\n",
    "La 12ème colonne contient le nombre de points obtenus.\n",
    "La 13ème colonne contient une variable qualitative qui précise l'épreuve (JO ou Décastar) concernée.\n",
    "\n",
    "Nous allons examiner ces données en commençant avec la librairie `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('data/decathlon.txt', sep=\"\\t\")  # lire les données dans un dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "__Alternativement :__ Si vous avez besoin de télécharger le fichier (par exemple sur colab) décommentez les deux lignes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems/main/data/decathlon.txt\n",
    "\n",
    "# my_data = pd.read_csv('decathlon.txt', sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Une __matrice de nuages de points__ est une visualisation en k x k panneaux des relations deux à deux entre k variables :\n",
    "* sur la diagonale, l'histogramme pour chacune des variables \n",
    "* hors diagonale, les nuages de points entre deux variables (non standardisées).\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "A vous d'afficher la visualisation à l'aide de la fonction `scatter_matrix` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "### DEBUT DE VOTRE CODE\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Vous pouvez aussi limiter la visualisation à quelques variables, pour des observations plus claires. Affichez la scatter_matrix pour les 3 ou 4 variables qui vous semblent les plus corrélées par exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Alternativement, la librairie `seaborn` permet des visualisations plus élaborées que `matplotlib`. Vous pouvez par exemple explorer les capacités de `jointplot`. \n",
    "https://seaborn.pydata.org/generated/seaborn.jointplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              height=6, space=0, color='b')\n",
    "\n",
    "sns.jointplot(x='Rank', y='Points', data = my_data, \n",
    "              kind='reg', height=6, space=0, color='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Nous allons maintenant effectuer une analyse en composantes principales des scores aux 10 épreuves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Commençons par extraire les variables prédictives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(my_data.drop(columns=['Points', 'Rank', 'Competition']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Standardisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Après visualisation des données, on peut remarquer des échelles et des distributions de données différentes selon les variables. \n",
    "On réapplique donc ici la procédure vue dans les TPs précédents pour standardiser nos données :  nous avons besoin d'un objet `StandardScaler` contenu dans le module `preprocessing` de `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "# Import du module\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation de l'objet StandardScaler\n",
    "std_scaler = ...\n",
    "\n",
    "# Ajustement de l'objet sur les données\n",
    "...\n",
    "\n",
    "# Transformation des données\n",
    "X_scaled = ...\n",
    "\n",
    "### FIN DE VOTRE CODE\n",
    "\n",
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Calcul des composantes principales\n",
    "\n",
    "Les algorithmes de factorisation de matrice de `scikit-learn` sont inclus dans le module `decomposition`. Pour  l'ACP, référez-vous à : \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Remarque : Nous avons ici peu de variables et pouvons nous permettre de calculer toutes les PC. \n",
    "\n",
    "La plupart des algorithmes implémentés dans `scikit-learn` suivent le fonctionnement suivant : \n",
    "* on instancie un objet, correspondant à un type d'algorithme/modèle, avec ses hyperparamètres (ici le nombre de composantes)\n",
    "* on utilise la méthode `fit` pour passer les données à cet algorithme\n",
    "* les paramètres appris sont maintenant accessibles comme arguments de cet objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet PCA pour 10 composantes principales\n",
    "\n",
    "### DEBUT DE VOTRE CODE\n",
    "pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On passe maintenant les données standardisées à cet objet\n",
    "# C'est ici que se font les calculs\n",
    "...\n",
    "\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Proportion de variance expliquée par les PCs\n",
    "\n",
    "Nous allons maintenant afficher la proportion de variance expliquée par les différentes composantes. Il est accessible dans le paramètre `explained_variance_ratio_` de notre objet `pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.ylabel(\"Proportion de variance expliquée\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Nous pouvons aussi afficher la proportion *cumulative* de variance expliquée, avec la fonction [`cumsum`](https://numpy.org/doc/2.1/reference/generated/numpy.cumsum.html) de `numpy` (importé plus haut sous l'alias `np`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Affichez sur un graphique similaire à celui ci-dessus, la proportion cumulative de variance expliquée en fonction du nombre de composantes principales considérées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "__Questions :__ \n",
    "* Quelle est la proportion de variance expliquée par les deux premières composantes ? \n",
    "* Combien de composantes faudrait-il utiliser pour expliquer 80% de la variance des données ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Projection des données sur les deux premières composantes principales\n",
    "\n",
    "Nous allons maintenant utiliser uniquement les deux premières composantes principales.\n",
    "\n",
    "Commençons par calculer la nouvelle représentation des données, c'est-à-dire leur projection sur ces deux PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "On peut afficher un nuage de points représentant les données selon ces deux PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "On peut maintenant colorer chaque point du nuage de points ci-dessus en fonction du classement de l'athlète qu'il représente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=my_data['Rank'])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "__Question :__ Qu'en conclure sur l'interprétation de la PC1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Interprétation des deux premières composantes principales\n",
    "Chaque composante principale est une combinaison linéaire des variables décrivant les données. Les poids de cette combinaison linéaire sont accesibles dans `pca.components_`.\n",
    "\n",
    "Nous pouvons maintenant visualiser non pas les individus comme ci-dessus, mais les 10 variables dans l'espace des 2 composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "print(pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(pcs[0], pcs[1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(pcs[0], pcs[1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "__Question :__ Quelles variables ont des contributions très similaires aux deux composantes principales ? Qu'en déduire sur leur similarité ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "__Question :__ Comment interpréter le signe des contributions des variables à la première composantes principales ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 2. Données « Olivetti »"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser la réduction de dimension pour représenter en deux dimensions un jeu de données contenant des visages. Il s'agit d'un jeu de données classique, contenant 400 photos de 64 par 64 pixels. Il s'agit de photos des visages de 40 personnes différentes (10 photos par personne), étiquetées par un numéro de classe entre 0 et 39 identifiant la personne.\n",
    "\n",
    "Nous pouvons charger ce jeu de données directement grâce à scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "__Si vous n'arrivez pas à télécharger les données :__\n",
    "* Aller sur : https://github.com/CroncLee/PCA-face-recognition/blob/master/olivetti_py3.pkz\n",
    "* Télécharger le fichier (bouton Download) \n",
    "utiliser la commande\n",
    "```\n",
    "    data = datasets.fetch_olivetti_faces(data_home=\"<PATH TO DATA>\")\n",
    "```\n",
    "En remplaçant <PATH TO DATA> par le chemin vers le dossier où vous avez enregistré les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Les données contiennent %d classes\" % len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Chaque image est représentée par une valeur (niveau de gris) pour chacun de ses pixels. \n",
    "\n",
    "Nous pouvons visualiser ces images à condition de réorganiser ces valeurs (= un vecteur de longueur 4096) en matrices 64x64. Par exemle ci-dessous pour l'image à l'index 23 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[77, :].reshape((64, 64)), interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA\n",
    "\n",
    "Commençons par une analyse en composantes principales comme à la section précédente :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "X_transformed_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Chaque image est maintenant représentée par non pas 4096 variables, mais par deux variables. Nous pouvons les visualiser en nuage de point, et les colorer par classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_pca[:, 0], X_transformed_pca[:, 1], c=y)\n",
    "plt.xlabel(\"Première composante principale\")\n",
    "plt.ylabel(\"Deuxième composante principale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "__Question :__ les images du même visage (= de la même classe) ont-elles des représentations proches ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Nous pouvons visualiser la contribution de chaque pixel à la première composante principale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[0, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Puis la contribution de chaque pixel à la deuxième composante principale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[1, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "**Question :** Quelle interprétation pouvons nous faire à partir de ces deux images représentant les contributions de chaque pixel sur les deux composantes principales ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### tSNE\n",
    "\n",
    "Essayons une autre méthode de réduction de la dimensionnalité pour tenter de mieux séparer nos classes\n",
    "\n",
    "Nous allons maintenant utiliser la même démarche que pour la PCA, mais avec l'algorithme tSNE, grâce à la classe [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) du module `manifold`.\n",
    "\n",
    "A vous :\n",
    "- de créer l'objet `tsne` à partir de la classe citée ci-dessus. On veut ici ramener notre jeu de données en deux dimensions pour le visualiser\n",
    "- d'intégrer les informations de notre jeu de données à cet objet\n",
    "- de transformer nos données pour obtenir leurs nouvelles coordonnées en deux dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "tsne = ...\n",
    "X_transformed = ...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Affichons le résultat de la réduction de dimensions avec tSNE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### Influence du paramètre de perplexité \n",
    "\n",
    "Le principal hyperparamètre influant sur la représentation obtenue par l'algorithme tSNE est le paramètre de perplexité. Celui-ci représente le nombre de voisins pour lesquels les distances sont préservées. Cela a donc une influence sur la conservation de la structure locale (perplexité faible) ou globale (perplexité élevée). La représentation obtenue peut varier très fortement en fonction de ce paramètre.\n",
    "\n",
    "Testez différentes valeurs du paramètre de perplexité et affichez les résultats correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "tsne_low_perp = ...\n",
    "X_transformed_low_perp = ...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_low_perp[:, 0], X_transformed_low_perp[:, 1], c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\")\n",
    "plt.title(\"tSNE (faible perplexité)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "tsne_high_perp = ...\n",
    "X_transformed_high_perp = ...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_high_perp[:, 0], X_transformed_high_perp[:, 1], c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\")\n",
    "plt.title(\"tSNE (perplexité élevée)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "Générons trois jeux de données en deux dimensions:\n",
    "- quatre clusters séparés issus de distributions normales\n",
    "- deux demi-cercles imbriqués (ou \"demi-lunes\")\n",
    "- deux cercles concentriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de points\n",
    "n_samples = 1000\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(23)\n",
    "\n",
    "four_blobs, four_blobs_labels = datasets.make_blobs(n_samples=n_samples, centers=4, n_features=2, random_state=170)\n",
    "\n",
    "moons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=170)\n",
    "\n",
    "circles, circles_labels = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "Visualisons ces données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "ax[0].scatter(four_blobs[:, 0], four_blobs[:, 1], c=four_blobs_labels, s=20, alpha=0.7)\n",
    "ax[1].scatter(moons[:, 0], moons[:, 1], c=moons_labels, s=20, alpha=0.7)\n",
    "ax[2].scatter(circles[:, 0], circles[:, 1], c=circles_labels, s=20, alpha=0.7)\n",
    "\n",
    "ax[0].set_title('4 clusters')\n",
    "ax[1].set_title('Lunes')\n",
    "ax[2].set_title('Cercles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Supposons maintenant que nous ne disposons **pas** des étiquettes. Quels algorithmes de clustering permettent de retrouver les clusters correspondant respectivement aux quatre blobs, deux lunes et deux cercles ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Algorithme des k-moyennes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "L'objectif de l'algorithme k-means est de retrouver $K$ clusters (et leur centroïde $\\mu_k$) de manière à **minimiser la variance intra-cluster** :\n",
    "\n",
    "\\begin{align}\n",
    "V = \\sum_{k = 1}^{K} \\sum_{x \\in C_k} \\frac{1}{|C_k|} (\\|x - \\mu_k\\|^2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "**Implémentation à la main**\n",
    "\n",
    "Nous allons commencer par implémenter l'algorithme \"à la main\", étape par étape, pour bien comprendre et visualiser ce que fait l'algorithme. Nous verrons ensuite comment utiliser directement l'implémentation du K-means dans la librairie `sklearn`. Les différentes étapes de l'algorithme sont : \n",
    "1. Sélectionner un nombre `k` de clusters (hyperparamètre)\n",
    "2. Initialiser aléatoirement les `k` centroïdes parmi nos points de données\n",
    "3. Calculer les distances de tous les points à ces centroïdes\n",
    "4. Assigner à chaque point le cluster du centroïde le plus proche\n",
    "5. Calculer la position des nouveaux centroïdes\n",
    "6. Répéter les étapes 3 à 5 jusqu'à convergence, c'est-à-dire jusqu'à ce que les centroïdes ne changent plus d'une itération à l'autre\n",
    "\n",
    "Essayons donc cet algorithme sur le dataset des 4 blobs, en commençant par choisir `k` et en prenant `k` points au hasard dans notre dataset qui constitueront les centroïdes initiaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CHOIX DE PARAMETRE\n",
    "k = ...\n",
    "\n",
    "random_indices = np.random.choice(len(four_blobs), k, replace=False)\n",
    "centroids_step0 = four_blobs[random_indices]\n",
    "\n",
    "for i, centroid in enumerate(centroids_step0):\n",
    "    print(f\"Centroïde {i} : x = {centroid[0]}, y = {centroid[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Visualisons les données avec ces centroïdes initiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.scatter(four_blobs[:, 0], four_blobs[:, 1], c='grey', s=20, alpha=0.7)\n",
    "ax.scatter(centroids_step0[:, 0], centroids_step0[:, 1], c='red', marker='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Calculons maintenant les distances de chaque point de donnée à ces centroïdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance between data point and centroid\n",
    "def compute_distance(data_point, centroid):\n",
    "    dist = np.sqrt(np.sum((data_point - centroid)**2))\n",
    "    return dist\n",
    "\n",
    "def compute_all_distances(dataset, centroids):\n",
    "    # Initialize distances array\n",
    "    distances = np.zeros((k, n_samples))  # k and n_samples defined above\n",
    "    \n",
    "    # DEBUT DE VOTRE CODE\n",
    "\n",
    "    # Calculate distance from each point to each centroid\n",
    "    for i in ...:\n",
    "        for j in ...:\n",
    "            distances[i, j] = ...\n",
    "\n",
    "    # FIN DE VOTRE CODE\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_all_distances(four_blobs, centroids_step0)\n",
    "\n",
    "# Exemple de distance des 5 premiers points aux k centroïdes\n",
    "print(\"Distances :\")\n",
    "print(\"\\t\\t\", \" \\t\\t\".join([f\"Point {i+1}\" for i in range(5)]))\n",
    "for i in range(k):\n",
    "    print(f\"Centroïde {i}\\t\", \"\\t\".join(distances[i, :5].astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "Nous devons maintenant assigner à chaque point le cluster correspondant au centroïde le plus proche. On utilise pour ça la fonction `argmin` de `numpy`. Cela constitue en quelques sortes nos étiquettes prédites de façon intermédiaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster(distances):\n",
    "    assignments = np.argmin(distances, axis=0)\n",
    "    return assignments\n",
    "\n",
    "intermediate_labels = assign_cluster(distances)\n",
    "print(intermediate_labels[:5])  # exemples d'étiquettes intermédiaires assignées aux points de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "On peut d'ailleurs vérifier que les labels intermédiaires assignés correspondent bien au centroïde le plus proche en comparant aux distances calculées plus haut pour ces 5 points.\n",
    "\n",
    "Nous allons maintenant visualiser ces clusters intermédiaires et les centroïdes initiaux sur un scatter plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_kmeans(dataset, labels, centroids, ax):\n",
    "\n",
    "    ax.scatter(dataset[:, 0], dataset[:, 1], c=labels, s=20, alpha=0.7)\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step0, ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Il faut maintenant calculer la position des nouveaux centroïdes, que l'on va afficher sur le graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_centroids(dataset, labels):\n",
    "    centroids = np.zeros((k, dataset.shape[1]))\n",
    "\n",
    "    for i in range(k):\n",
    "        centroids[i] = dataset[labels == i].mean(axis=0)\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "centroids_step1 = compute_new_centroids(four_blobs, intermediate_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step0, axes[0])\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step1, axes[1])\n",
    "\n",
    "axes[0].set_title(\"Centroïdes initiaux\")\n",
    "axes[1].set_title(\"Centroïdes après une itération\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "On recommence maintenant les différentes étapes : calcul de distance aux centroïdes, assignation des clusters aux points de données et calcul des nouveaux centroïdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(24, 4))\n",
    "n_iter = 10  # Nombre d'itérations de l'algorithme\n",
    "\n",
    "current_fig = 0\n",
    "\n",
    "centroids = centroids_step1\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # DEBUT DE VOTRE CODE\n",
    "\n",
    "    distances = ...\n",
    "    intermediate_labels = ...\n",
    "\n",
    "    # On affiche la visualisation toutes les 2 itérations\n",
    "    if (i+1) % 2 == 0:\n",
    "        visualise_kmeans(four_blobs, intermediate_labels, centroids, axes[current_fig])\n",
    "        axes[current_fig].set_title(f\"Itération {i+1}\")\n",
    "        current_fig += 1\n",
    "        \n",
    "    centroids = ...\n",
    "\n",
    "    # FIN DE VOTRE CODE\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "On voit bien ici comment, par itérations successives, l'algorithme est capable d'identifier correctement nos clusters, en déplaçant petit à petit les centroïdes et en réajustant l'appartenance de nos points de données. \n",
    "\n",
    "**Question** : Dans quel(s) cas l'algorithme peut-il donner de mauvais résultats ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "**Implémentation avec `sklearn`**\n",
    "\n",
    "Nous allons maintenant reprendre les 3 datasets créés plus haut (4 clusters, Demi-lunes et cercles concentriques), et leur appliquer l'algorithme implémenté dans la librairie sklearn, qui reprend les étapes que nous venons de voir.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUT DE VOTRE CODE\n",
    "# Initialisation de trois modèles k-means avec le nombre\n",
    "# de clusters théoriquement attendu (resp. 4, 2 et 2):\n",
    "kmeans_four_blobs = ...\n",
    "kmeans_moons = ...\n",
    "kmeans_circles = ...\n",
    "\n",
    "# Application aux données \n",
    "...\n",
    "...\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "L'attribut `.labels_` contient, pour chaque observation, le numéro du cluster auquel cette observation est assignée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_four_blobs.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "Voyons maintenant à quoi ressemble le clustering obtenu pour nos trois datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du clustering\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "ax[0].scatter(four_blobs[:, 0], four_blobs[:, 1], c=kmeans_four_blobs.labels_)\n",
    "ax[1].scatter(moons[:, 0], moons[:, 1], c=kmeans_moons.labels_)\n",
    "ax[2].scatter(circles[:, 0], circles[:, 1], c=kmeans_circles.labels_)\n",
    "\n",
    "ax[0].set_title('4 clusters (k=4)')\n",
    "ax[1].set_title('Lunes (k=2)')\n",
    "ax[2].set_title('Cercles (k=2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "**Questions :** Est-ce le clustering espéré ? Dans quels cas l'algorithme des k-moyennes fonctionne-t-il correctement ? Pourquoi ne fonctionne-t-il pas dans les autres cas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "#### Trouver $K$ avec le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "Souvent, le nombre de clusters exact, $K$, n'est pas connu à l'avance. Nous pouvons tout de même appliquer l'algorithme k-means et mesurer la performance du clustering pour trouver le meilleur paramètre $K$. L'une des métriques utilisées est le **coefficient de silhouette**.\n",
    "\n",
    "Le coefficient (ou score) de silhouette permet de **comparer les distances moyennes intra- et inter-cluster** :\n",
    "\n",
    "\\begin{align}\n",
    "\\text{score} = \\frac{b - a}{\\max(a, b)}\n",
    "\\end{align}\n",
    "\n",
    "avec $a$ la distance moyenne intra-cluster et $b$ la distance d'un point au cluster étranger le plus proche. Le score se calcule par observation (avec une valeur entre -1 et 1) puis la moyenne de ce score permet d'évaluer le clustering du nuage de point dans son ensemble.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"4 clusters: Coefficient de silhouette pour le k-means (k=4) : %.2f\" % \n",
    "      metrics.silhouette_score(four_blobs, kmeans_four_blobs.labels_))\n",
    "print(f\"Lunes: Coefficient de silhouette pour le k-means (k=2) : %.2f\" % \n",
    "      metrics.silhouette_score(moons, kmeans_moons.labels_))\n",
    "print(f\"Cercles: Coefficient de silhouette pour le k-means (k=2) : %.2f\" % \n",
    "      metrics.silhouette_score(circles, kmeans_circles.labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "Essayons d'évaluer la performance du clustering en fonction du nombre $K$ de clusters, en le faisant varier dans la fourchette [2, .., 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 9)\n",
    "names = ['4 clusters', 'Lunes', 'Cercles']\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for i, dataset in enumerate([four_blobs, moons, circles]):\n",
    "    silhouettes = []\n",
    "    \n",
    "    for kval in k_values:\n",
    "\n",
    "        ### DEBUT DE VOTRE CODE\n",
    "        # Initialisez un modèle KMeans avec le nombre de clusters testé:\n",
    "        kmeans_k = ...\n",
    "        \n",
    "        # Entraînez le modèle sur les données\n",
    "        ...\n",
    "        \n",
    "        # Ajoutez le score de silhouette obtenu à la liste\n",
    "        ...\n",
    "        \n",
    "        ### FIN DE VOTRE CODE\n",
    "    \n",
    "    # Visualisation du score de silhouette\n",
    "    ax[0,i].plot(k_values, silhouettes)\n",
    "    ax[0,i].set_xlabel(\"K\")\n",
    "    ax[0,i].set_ylabel(\"Silhouette score\")\n",
    "    ax[0,i].set_title(names[i])\n",
    "    \n",
    "    print(\"Dataset:\", names[i])\n",
    "    best_silhouette = np.max(silhouettes)\n",
    "    print(\"Coefficient de Silhouette optimal : %.2f\" % best_silhouette)\n",
    "    best_K = k_values[silhouettes.index(best_silhouette)]\n",
    "    print(\"Nombre de clusters K correspondant: %.0f\" % best_K)\n",
    "    \n",
    "    \n",
    "    kmeans_k = cluster.KMeans(n_clusters=best_K)\n",
    "    kmeans_k.fit(dataset)\n",
    "    ax[1,i].scatter(dataset[:, 0], dataset[:, 1], c=kmeans_k.labels_)\n",
    "    ax[1,i].set_xlabel('x1')\n",
    "    ax[1,i].set_ylabel('x2')\n",
    "    ax[1,i].set_title('Clustering avec ' + str(best_K) + ' clusters')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "**Conclusions :** \n",
    "- L'algorithme des k-moyennes permet d'obtenir un clustering satisfaisant pour le dataset avec quatre blobs bien séparés, y compris sans connaître le nombre idéal de clusters à l'avance, auquel cas le coefficient de silhouette permet de retrouver ce nombre idéal.\n",
    "- En revanche, malgré l'optimisation du score de silhouette, cet algorithme ne permet pas d'obtenir de bons résultats pour les autres jeux de données, que ce soit pour les deux lunes imbriquées ou les deux cercles concentriques.\n",
    "\n",
    "Nous allons donc maintenant essayer un autre algorithme de clustering et tester la performance de celui-ci pour la comparer au k-means. Nous allons nous limiter aux deux datasets pour lesquels l'algorithme k-means ne fonctionne pas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN (Clustering par densité)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "L'algorithme DBSCAN (Density-Based Spatial Clustering of Applications with Noise) fonctionne en deux temps :\n",
    "- Toutes les observations suffisamment proches sont connectées entre elles.\n",
    "- Les observations avec un nombre minimal de voisins connectés sont considérées comme des *core samples*, à partir desquelles les clusters sont étendues. **Toutes les observations suffisamment proche d'un *core sample* appartiennent au même cluster que celui-ci**. \n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "L'algorithme DBSCAN prend en entrée deux hyperparamètres :\n",
    "- `eps` : la *taille du voisinage*, autrement dit la distance entre deux points de données en-dessous de laquelle un point est considéré à l'intérieur du voisinage de l'autre.\n",
    "\n",
    "- `min_samples` : le nombre de voisins minimum pour qu'un point de données soit considéré comme un *core sample*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "# Initialisation de deux modèles DBSCAN\n",
    "# avec les hyperparamètres eps=0.2, min_samples=2:\n",
    "dbscan_moons = ...\n",
    "dbscan_circles = ...\n",
    "\n",
    "# Application aux données \n",
    "...\n",
    "...\n",
    "### FIN DE VOTRE CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "L'attribut `.labels_` contient, pour chaque observation, le numéro du cluster auquel cette observation est assignée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre d'étiquettes pour le dataset moons:\", len(np.unique(dbscan_moons.labels_)))\n",
    "print(\"Nombre d'étiquettes pour le dataset circles:\", len(np.unique(dbscan_circles.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "Visualisons les clusters obtenus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].scatter(moons[:, 0], moons[:, 1], c=dbscan_moons.labels_)\n",
    "ax[0].set_title(\"Clustering DBSCAN (eps=0.2)\")\n",
    "\n",
    "ax[1].scatter(circles[:, 0], circles[:, 1], c=dbscan_circles.labels_)\n",
    "ax[1].set_title(\"Clustering DBSCAN (eps=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "On peut voir ici que l'algorithme DBSCAN est capable d'identifier les deux clusters respectifs dans les deux datasets qui posaient problème à l'algorithme k-means. \n",
    "\n",
    "On peut noter également que nous n'avons pas eu besoin de renseigner un nombre de clusters à priori pour que l'algorithme identifie correctement le bon nombre de clusters. En revanche, l'algorithme est sensible aux deux hyperparamètre cités plus haut, ce que nous allons évaluer maintenant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "#### Rôle du paramètre de taille de voisinage (`eps`)\n",
    "\n",
    "Nous allons évaluer l'influence du paramètre `eps` sur le jeu de données des cercles concentriques (`circles`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "# Initialisation d'un clustering DBSCAN avec eps petit (ex, 0.05) et d'un autre avec eps grand (ex, 2.0):\n",
    "dbscan_low = ...\n",
    "dbscan_high = ...\n",
    "\n",
    "# Application aux données \n",
    "...\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "**Question :** Quel est le nombre de clusters obtenus dans nos deux modèles ? (Utiliser l'attribut `.labels`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUT DE VOTRE CODE\n",
    "...\n",
    "...\n",
    "### FIN DE VOTRE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "outliers = np.where(dbscan_low.labels_ == -1)[0]\n",
    "plt.scatter(circles[outliers, 0], circles[outliers, 1], marker='*', color='red')\n",
    "\n",
    "non_outliers = np.where(dbscan_low.labels_ != -1)[0]\n",
    "plt.scatter(circles[non_outliers, 0], circles[non_outliers, 1], c=dbscan_low.labels_[non_outliers])\n",
    "plt.title(\"Clustering DBSCAN (eps=0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=dbscan_high.labels_)\n",
    "plt.title(\"Clustering DBSCAN (eps=2.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Trouver eps avec le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient de silhouette pour DBSCAN (eps=0.2) : %.2f\" % \n",
    "      metrics.silhouette_score(circles, dbscan_circles.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.logspace(-3, 1, 40)\n",
    "silhouettes = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan_eps = cluster.DBSCAN(eps=eps, min_samples=2)\n",
    "    dbscan_eps.fit(circles)\n",
    "    if len(np.unique(dbscan_eps.labels_)) > 1: # nécessaire pour calculer le coeff de silhouette\n",
    "        silhouettes.append(metrics.silhouette_score(circles, dbscan_eps.labels_))\n",
    "    else:\n",
    "        silhouettes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_values, silhouettes)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eps (échelle log)\")\n",
    "plt.ylabel(\"silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = np.max(silhouettes)\n",
    "print(\"Coefficient de silhouette optimal : %.2f\" % best_silhouette)\n",
    "print(\"Eps correspondant : %.2f\" % eps_values[silhouettes.index(best_silhouette)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "Voyons ce que donne le clustering obtenu en utilisant le paramètre `eps` avec lequel on obtient le meilleur coefficient de silhouette :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eps = eps_values[silhouettes.index(best_silhouette)]\n",
    "\n",
    "dbscan_best = cluster.DBSCAN(eps=best_eps, min_samples=2)\n",
    "dbscan_best.fit(circles)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=dbscan_best.labels_)\n",
    "plt.title(\"Clustering DBSCAN (eps=%.2f)\" % best_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "**Question :**  Quel est le problème ici ? Le coefficient de silhouette est-il adapté à notre dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "### Index de Rand ajusté"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "L'index de Rand ajusté permet de **comparer un résultat de clustering avec des étiquettes**. Pour chaque paire d'observations nous regardons si elles se situent dans le même cluster ou non, dans le clustering prédit et réel. L'index prend des valeurs entre 0 (clustering aléatoire) et 1 (clustering parfait).\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "__Question :__ Pourquoi ne pas utiliser une métrique d'évaluation de modèle de classification ici ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté du K-means (K=2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, kmeans_circles.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index de Rand ajusté de dbscan (eps=0.2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, dbscan_circles.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "## Bonus: Clustering sur les Manchots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "Le but est ici de tester plusieurs méthodes non-supervisées de clustering sur un nouveau dataset, et de les comparer. Essayez les méthodes vues plus haut telles que [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) ou [sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). Vous pouvez aussi essayer d'autres méthodes telles que le mélange de Gaussiennes ([sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)).\n",
    "\n",
    "Quelles méthodes devraient mieux fonctionner selon vous ? Pourquoi ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "Le jeu de données que nous vous proposons d'utiliser ici concerne différentes espèces de manchots et certaines caractéristiques physiques. Les 3 espèces sont : les manchots Adélie, les manchots papou (gentoo) et les manchots à jugulaire (chinstrap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins = pd.read_csv(\"data/penguins_data.csv\")\n",
    "palmerpenguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "__Alternativement :__ Si vous avez besoin de télécharger le fichier (par exemple sur colab), décommentez les deux lignes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems/main/data/penguins_data.csv\n",
    "\n",
    "# palmerpenguins = pd.read_csv(\"penguins_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "La première étape est de filtrer certaines données pour éviter les données manquantes (NA ou NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins = palmerpenguins[palmerpenguins['bill_depth_mm'].notna()]\n",
    "palmerpenguins = palmerpenguins.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "On va se concentrer ici uniquement sur deux caractéristiques de nos manchots : la longueur du bec et leur poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X = np.array(palmerpenguins[[\"bill_length_mm\", \"body_mass_g\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "Nos deux variables ne sont pas du tout à la même échelle, comme on peut le voir dans les données affichées plus haut. Il faut donc standardiser nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation (centrer-réduire)\n",
    "penguins_X = preprocessing.StandardScaler().fit_transform(penguins_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names, species_int = np.unique(palmerpenguins.species, return_inverse=True)\n",
    "penguins_labels = species_int\n",
    "species_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "Essayons d'abord de visualiser nos données sur un graphique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels)\n",
    "plt.xlabel(\"bill_length_mm (centrée-réduite)\")\n",
    "plt.ylabel(\"body_mass_g (centrée-réduite)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "A vous maintenant de tester différents algorithmes de clustering sur ces données, et d'en évaluer les performances. Pourrez-vous obtenir un clustering parfait ? \n",
    "\n",
    "En plus des algorithmes de clustering utilisés ci-dessus, vous pouvez essayer le mélange de Gaussiennes ([GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)) par exemple, ou d'autres méthodes des modules `cluster` ou `mixture`.\n",
    "Donnez pour chacun les coefficients de silhouette et indices de Rand ajusté que vous obtenez, ainsi qu'une visualisation du clustering obtenu. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "### Modèle de mélange gaussien "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "Le modèle de mélange de gaussiennes cherche à **optimiser les paramètres d'un nombre fini de gaussiennes** aux données. \n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
