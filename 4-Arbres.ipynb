{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 4 : Arbres et méthodes ensemblistes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Notebook préparé par [Chloé-Agathe Azencott](http://cazencott.info).\n",
    "\n",
    "Dans ce notebook il s'agit de découvrir les arbres de décision et les méthodes ensemblistes (forêts aléatoires, boosting de gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger numpy as np, matplotlib as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # règle la taille de police globalement pour les plots (en pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Le but de ce notebook est d'utiliser la description visuelle d'un champignon pour prédire s'il est comestible ou non. \n",
    "\n",
    "Les données sont disponibles dans `data/mushrooms.csv`. Elles sont issues du jeu de données https://archive.ics.uci.edu/ml/datasets/Mushroom mais légèrement modifiées.\n",
    "\n",
    "Il contient une première ligne (header) décrivant les colonnes, puis une ligne par champignon. Les valeurs des différentes variables sont toutes représentées par des lettres ; en voici la signification :\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n",
    "2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\n",
    "4. bruises: bruises=t,no=f\n",
    "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n",
    "6. gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "7. gill-spacing: close=c,crowded=w,distant=d\n",
    "8. gill-size: broad=b,narrow=n\n",
    "9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y\n",
    "10. stalk-shape: enlarging=e,tapering=t\n",
    "11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?\n",
    "12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n",
    "15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n",
    "16. veil-type: partial=p,universal=u\n",
    "17. veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "18. ring-number: none=n,one=o,two=t\n",
    "19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z\n",
    "20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y\n",
    "21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\n",
    "22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n",
    "\n",
    "La première colomne nous informe de la classe de chaque champignon, 'e' pour comestible (edible) et 'p' pour vénéneux (poisonous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "__Alternativement :__ Si vous avez besoin de télécharger le fichier (par exemple sur colab) :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9",
   "metadata": {},
   "source": [
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems/main/data/mushrooms.csv\n",
    "\n",
    "df = pd.read_csv(\"mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Conversion des variables en valeurs numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Nos variables sont pour l'instant _catégorielles._ \n",
    "\n",
    "Par exemple, pour la variable \"forme du chapeau\" (`cap shape`), `b` correspond à un chapeau campanulé (bell cap), `c` à un chapeau conique (conical cap), `f` à un chapeau plat (flat cap), `k` à un chapeau papillé (knobbed cap), `s` à un chapeau déprimé (sunken cap), and `x` à un chapeau convexe. \n",
    "\n",
    "Pour travailler avec ces données, il nous faut convertir ces catégories en valeurs numériques. \n",
    "\n",
    "Une possibilité est de convertir chaque lettre en un nombre entre 0 et le nombre total de catégorie, grâce à [preprocessing.LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\n",
    "\n",
    "Cet encodage n'est pas nécessairement idéal : un algorithme qui utilise la distance euclidienne va considérer qu'un chapeau convexe (`x` converti en 5) est plus proche d'un chapeau déprimé (`s` converti en 4) que d'un chapeau conique (`c` converti en 1), ce qui n'a pas beaucoup de sens. Cela ne pose cependant pas de problème pour les algorithmes basés sur les arbres de décision, qui traitent les catégories comme telles et non pas comme des valeurs numériques. La conversion est uniquement nécessaire pour des raisons d'implémentation.\n",
    "\n",
    "L'encodage [one-hot](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features) est généralement un meilleur choix. Remarquez néanmoins qu'il a l'inconvénient d'augmenter le nombre de variables, et de créer des variables corrélées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Nous pouvons de nouveau observer nos données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Création des matrices X et y de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "__Question :__ Combien d'échantillons (examples) notre jeu de données contient-il ? Combien de variables ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 2. Cadre de sélection et évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant séparer nos données en un jeu d'entrainement et un jeu de test, puis fixer une séparation du jeu d'entrainement en 10 folds pour pouvoir faire des validations croisées dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Jeu d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y,\n",
    "                                    test_size=0.3, # 30% des données dans le jeu de test\n",
    "                                    random_state=42 # graine du générateur aléatoire\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "# Créer un objet KFold qui permettra de cross-valider en n_folds folds\n",
    "kf = model_selection.KFold(n_splits=n_folds,  \n",
    "                           shuffle=True # mélanger les échantillons avant de créer les folds\n",
    "                          )\n",
    "\n",
    "# Utiliser kf pour partager le jeu d'entraînement en n_folds folds. \n",
    "# kf.split retourne un iterateur (consommé après une boucle).\n",
    "# Pour pouvoir se servir plusieurs fois des mêmes folds, nous transformons cet itérateur en liste d'indices :\n",
    "kf_indices = list(kf.split(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 3. Arbre de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser un arbre de décision pour apprendre un classifieur sur nos données.\n",
    "\n",
    "Les arbres de décision sont implémentés dans la classe [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) du module `tree` de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Arbre de décision avec les hyperparamètres par défaut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Déterminons le score F1 en validation croisée d'un arbre de décision dont les hyperparamètres prennent les valeurs par défaut dans scikit-learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree_default = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_tree_default = model_selection.cross_val_score(model_tree_default, # prédicteur à évaluer\n",
    "                                                  X_train, y_train, # données d'entrainement\n",
    "                                                  cv=kf_indices, # validation croisée à utiliser\n",
    "                                                  scoring='f1' # métrique d'évaluation de la performance\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"F1 d'un abre de décision (défaut) en validation croisée : %.3f +/- %.3f\" % (np.mean(f1_tree_default), np.std(f1_tree_default)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "__Question :__ Que pensez-vous de cette performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Validation croisée de la profondeur de l'arbre de décision\n",
    "\n",
    "Par défaut (voir la doc), nous avons utilisé un arbre de décision dont la profondeur est maximale. Nous allons maintenant considérer la profondeur de l'arbre (`max_depth`) comme un hyperparamètre à optimiser par une recherche en grille. Nous reprenons et adaptons le code utilisé pour les kNN dans le Notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "tags": []
   },
   "source": [
    "Commençons par définir la grille :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = np.arange(2, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet GridSearchCV\n",
    "grid_tree = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), # prédicteur à évaluer\n",
    "                                         {'max_depth': d_values}, # dictionnaire de valeurs d'hyperparamètres\n",
    "                                         cv=kf_indices, # validation croisée à utiliser\n",
    "                                         scoring='f1' # métrique d'évaluation de la performance\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Utilisation de cet objet sur les données d'entraînement\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "La valeur optimale de l'hyperparamètre est donnée par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Le code suivant permet d'afficher la performance du modèle selon la valeur de l'hyperparamètre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_score = grid_tree.cv_results_['mean_test_score']\n",
    "stde_test_score = grid_tree.cv_results_['std_test_score'] / np.sqrt(n_folds) # standard error\n",
    "\n",
    "plt.plot(d_values, mean_test_score)\n",
    "plt.plot(d_values, (mean_test_score + stde_test_score), '--', color='steelblue')\n",
    "plt.plot(d_values, (mean_test_score - stde_test_score), '--', color='steelblue')\n",
    "plt.fill_between(d_values, (mean_test_score + stde_test_score), \n",
    "                 (mean_test_score - stde_test_score), alpha=0.2)\n",
    "\n",
    "best_index = np.where(d_values == grid_tree.best_params_['max_depth'])[0][0]\n",
    "plt.scatter(d_values[best_index], mean_test_score[best_index])\n",
    "\n",
    "\n",
    "plt.xlabel('profondeur maximale')\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"Performance (en validation croisée) le long de la grille\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "__Question :__ Que pensez-vous de cette performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Arbre de décision optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur F1 en validation croisée : %.3f\" % grid_tree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "On peut maintenant récupérer l'arbre de décision optimal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tree_best = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## 4. Interprétation de l'arbre de décision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "La méthode [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) du module `tree` de scikit-learn nous permet de visualiser l'arbre de décision optimal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 20))\n",
    "tree.plot_tree(model_tree_best, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "__Question :__ Le modèle appris vous parait-il interprétable ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Importance des variables\n",
    "\n",
    "Pour interpréter l'arbre de décision, nous pouvons aussi regarder l'importance de chaque variable. Elle est d'autant plus grande que la variable permet de réduire l'erreur de classification de l'arbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Afficher les importances de l'arbre de décision\n",
    "plt.scatter(range(num_features), model_tree_best.feature_importances_,\n",
    "           label=\"Arbre de decision\")\n",
    "\n",
    "# Légende\n",
    "tmp = plt.legend(fontsize=14)\n",
    "\n",
    "# Axe des abcisses\n",
    "plt.xlabel('Variables', fontsize=14)\n",
    "feature_names = list(df.columns[1:])\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "\n",
    "# Axe des ordonnées\n",
    "tmp = plt.ylabel('Importance', fontsize=14)\n",
    "\n",
    "# Titre\n",
    "tmp = plt.title('Importance des variables', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Comparaison à une régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Nous pouvons aussi comparer ces importances aux coefficients de régression d'une régression logistique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = np.logspace(-3, 3, 50)\n",
    "\n",
    "# Centrer-réduire les données\n",
    "std_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = std_scaler.transform(X_train)\n",
    "X_test_scaled = std_scaler.transform(X_test)\n",
    "\n",
    "# Instanciation d'un objet GridSearchCV\n",
    "grid_logreg = model_selection.GridSearchCV(linear_model.LogisticRegression(penalty='l2'), # prédicteur à évaluer\n",
    "                                          {'C': c_values}, # dictionnaire de valeurs d'hyperparamètres\n",
    "                                          cv=kf_indices, # validation croisée à utiliser\n",
    "                                          scoring='f1' # métrique d'évaluation de la performance\n",
    "                                          )\n",
    "\n",
    "grid_logreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur F1 en validation croisée : %.3f\" % grid_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "__Question :__ Comparez cette performance à celle de l'arbre de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Ramener les importances entre 0 et 1\n",
    "tree_importances = model_tree_best.feature_importances_\n",
    "tree_importances_min = np.min(tree_importances)\n",
    "tree_importances_max = np.max(tree_importances)\n",
    "tree_importances = (tree_importances-tree_importances_min)/(tree_importances_max-tree_importances_min)\n",
    "\n",
    "# Afficher les importances de l'arbre de décision\n",
    "plt.bar(range(num_features), tree_importances,\n",
    "           label=\"Arbre de decision\", width=0.4)\n",
    "\n",
    "# Ramener les valeurs absolues des coefficients du modèle linéaire entre 0 et 1 \n",
    "logreg_coeffs = np.abs(grid_logreg.best_estimator_.coef_[0])\n",
    "logreg_coeffs_min = np.min(logreg_coeffs)\n",
    "logreg_coeffs_max = np.max(logreg_coeffs)\n",
    "logreg_coeffs = (logreg_coeffs-logreg_coeffs_min)/(logreg_coeffs_max-logreg_coeffs_min)\n",
    "\n",
    "# Afficher les importances de la regression logistique\n",
    "plt.bar((np.arange(num_features)+0.4), logreg_coeffs,\n",
    "           label=\"Régression logistique\", width=0.4)\n",
    "\n",
    "\n",
    "# Légende\n",
    "tmp = plt.legend(fontsize=14)\n",
    "\n",
    "# Axe des abcisses\n",
    "plt.xlabel('Variables', fontsize=14)\n",
    "feature_names = list(df.columns[1:])\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "\n",
    "# Axe des ordonnées\n",
    "tmp = plt.ylabel('Importance', fontsize=14)\n",
    "\n",
    "# Titre\n",
    "tmp = plt.title('Importance des variables', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "__Question :__ Comment ces importances se comparent-elles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## 5. Forêt aléatoire\n",
    "\n",
    "Peut-on améliorer les performances de l'arbre de décision en utilisant une méthode ensembliste ? Nous allons utiliser ici une forêt aléatoire, implémentée dans la classe [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) du module `ensemble` de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Validation croisée du nombre d'arbres et de la profondeur maximale de chacun d'entre eux.\n",
    "\n",
    "Nous allons maintenant considérer deux hyperparamètres, la profondeur maximale de chaque arbre (`max_depth`), et le nombre d'arbres dans la forêt (`n_estimators`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "tags": []
   },
   "source": [
    "Commençons par définir la grille :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = np.array([3, 4, 10])\n",
    "n_values = np.array([10, 20, 50, 100, 200])#, 100, 200, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet GridSearchCV\n",
    "grid_rf = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), # prédicteur à évaluer\n",
    "                                       {'max_depth': d_values, 'n_estimators': n_values}, # dictionnaire de valeurs d'hyperparamètres\n",
    "                                       cv=kf_indices, # validation croisée à utiliser\n",
    "                                       scoring='f1' # métrique d'évaluation de la performance\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Utilisation de cet objet sur les données d'entraînement\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "La valeur optimale des hyperparamètres est donnée par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "Et nous pouvons afficher la performance du modèle selon la valeur de chacun des deux hyperparamètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réorganiser les scores en tableau 2D\n",
    "mean_test_score_array = np.reshape(grid_rf.cv_results_['mean_test_score'], (len(d_values), len(n_values)))\n",
    "std_test_score_array = np.reshape(grid_rf.cv_results_['std_test_score'], (len(d_values), len(n_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, d) in enumerate(d_values):\n",
    "    mean_test_score = mean_test_score_array[idx, :]\n",
    "    stde_test_score = std_test_score_array[idx, :] / np.sqrt(n_folds) # standard error\n",
    "\n",
    "    p = plt.plot(n_values, mean_test_score, label=\"Profondeur max = %d\" % d)\n",
    "    plt.plot(n_values, (mean_test_score + stde_test_score), '--', color=p[0].get_color())\n",
    "    plt.plot(n_values, (mean_test_score - stde_test_score), '--', color=p[0].get_color())\n",
    "    plt.fill_between(n_values, (mean_test_score + stde_test_score), \n",
    "                     (mean_test_score - stde_test_score), alpha=0.2)\n",
    "    \n",
    "    # Afficher les meilleurs hyperparamètres\n",
    "    if d == grid_rf.best_params_['max_depth']:\n",
    "        best_ntree_index = np.where(n_values == grid_rf.best_params_['n_estimators'])[0][0]\n",
    "        plt.scatter(n_values[best_ntree_index], mean_test_score[best_ntree_index], \n",
    "                   marker='*', s=200, color='red')\n",
    "        \n",
    "plt.legend(loc=(1.1, 0))\n",
    "plt.xlabel(\"Nombre d'arbres\")\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"Performance (en validation croisée) le long de la grille\")\n",
    "plt.xscale('log') # utiliser une échelle logarithmique en abcisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "__Question :__ Comment la performance des forêts aléatoires se compare-t-elle aux performances précédentes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### Forêt aléatoire optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur F1 en validation croisée : %.3f\" % grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "On peut maintenant récupérer l'arbre de décision optimal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_best = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Importance des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Nous pouvons encore une fois regarder l'importance de chaque variable, pour le meilleur modèle de forêt aléatoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Ramener les importances entre 0 et 1\n",
    "tree_importances = model_tree_best.feature_importances_\n",
    "tree_importances_min = np.min(tree_importances)\n",
    "tree_importances_max = np.max(tree_importances)\n",
    "tree_importances = (tree_importances-tree_importances_min)/(tree_importances_max-tree_importances_min)\n",
    "\n",
    "# Afficher les importances de l'arbre de décision\n",
    "plt.bar(range(num_features), tree_importances,\n",
    "           label=\"Arbre de décision\", width=0.3)\n",
    "\n",
    "# Ramener les valeurs absolues des coefficients du modèle linéaire entre 0 et 1 \n",
    "logreg_coeffs = np.abs(grid_logreg.best_estimator_.coef_[0])\n",
    "logreg_coeffs_min = np.min(logreg_coeffs)\n",
    "logreg_coeffs_max = np.max(logreg_coeffs)\n",
    "logreg_coeffs = (logreg_coeffs-logreg_coeffs_min)/(logreg_coeffs_max-logreg_coeffs_min)\n",
    "\n",
    "# Afficher les importances de la regression logistique\n",
    "plt.bar((np.arange(num_features)+0.3), logreg_coeffs,\n",
    "           label=\"Régression logistique\", width=0.3)\n",
    "\n",
    "# Ramener les importances entre 0 et 1\n",
    "rf_importances = model_rf_best.feature_importances_\n",
    "rf_importances_min = np.min(rf_importances)\n",
    "rf_importances_max = np.max(rf_importances)\n",
    "rf_importances = (rf_importances-rf_importances_min)/(rf_importances_max-rf_importances_min)\n",
    "\n",
    "# Afficher les importances de l'arbre de décision\n",
    "plt.bar((np.arange(num_features)+0.6),  rf_importances,\n",
    "           label=\"Forêt aléatoire\", width=0.3)\n",
    "\n",
    "\n",
    "# Légende\n",
    "tmp = plt.legend()\n",
    "\n",
    "# Axe des abcisses\n",
    "plt.xlabel('Variables')\n",
    "feature_names = list(df.columns[1:])\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "\n",
    "# Axe des ordonnées\n",
    "tmp = plt.ylabel('Importance')\n",
    "\n",
    "# Titre\n",
    "tmp = plt.title('Importance des variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "__Question :__ Quelles sont maintenant les variables les plus importantes ? Comment cela se compare-t-il aux modèles précédents ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "## 6. Boosting de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Le boosting de gradient est implémenté dans scikit-learn dans la classe [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=boosting#sklearn.ensemble.GradientBoostingClassifier) du module `ensemble`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### Validation croisée et sélection des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Comme pour les forêts aléatoires, nous allons optimiser ici le nombre d'estimateurs et la profondeur des arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.array([10, 20, 50, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = np.array([3, 4, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet GridSearchCV\n",
    "grid_boost = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), # prédicteur à évaluer\n",
    "                                       {'max_depth' : d_values, 'n_estimators': n_values}, # dictionnaire de valeurs d'hyperparamètres\n",
    "                                       cv=kf_indices, # validation croisée à utiliser\n",
    "                                       scoring='f1' # métrique d'évaluation de la performance\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Utilisation de cet objet sur les données d'entraînement\n",
    "grid_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "La valeur optimale des hyperparamètres est donnée par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_boost.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Et nous pouvons afficher la performance du modèle selon la valeur de chacun des deux hyperparamètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réorganiser les scores en tableau 2D\n",
    "mean_test_score_array = np.reshape(grid_boost.cv_results_['mean_test_score'], (len(d_values), len(n_values)))\n",
    "std_test_score_array = np.reshape(grid_boost.cv_results_['std_test_score'], (len(d_values), len(n_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, d) in enumerate(d_values):\n",
    "    mean_test_score = mean_test_score_array[idx, :]\n",
    "    stde_test_score = std_test_score_array[idx, :] / np.sqrt(n_folds) # standard error\n",
    "\n",
    "    p = plt.plot(n_values, mean_test_score, label=\"Profondeur max = %d\" % d)\n",
    "    plt.plot(n_values, (mean_test_score + stde_test_score), '--', color=p[0].get_color())\n",
    "    plt.plot(n_values, (mean_test_score - stde_test_score), '--', color=p[0].get_color())\n",
    "    plt.fill_between(n_values, (mean_test_score + stde_test_score), \n",
    "                     (mean_test_score - stde_test_score), alpha=0.2)\n",
    "    \n",
    "    # Afficher les meilleurs hyperparamètres\n",
    "    if d == grid_boost.best_params_['max_depth']:\n",
    "        best_ntree_index = np.where(n_values == grid_boost.best_params_['n_estimators'])[0][0]\n",
    "        plt.scatter(n_values[best_ntree_index], mean_test_score[best_ntree_index], \n",
    "                   marker='*', s=200, color='red')\n",
    "        \n",
    "plt.legend(loc=(1.1, 0))\n",
    "plt.xlabel(\"Nombre d'arbres\")\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"Performance (en validation croisée) le long de la grille\")\n",
    "plt.xscale('log') # utiliser une échelle logarithmique en abcisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "__Question :__ Comment la performance du boosting de gradient évolue-t-elle en fonction des valeurd d'hyperparamètre ? Comment se compare-t-elle aux performances précédentes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "### Boosting optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur F1 en validation croisée : %.3f\" % grid_boost.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "On peut maintenant récupérer l'arbre de décision optimal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boost_best = grid_boost.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "### Importance des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "Nous pouvons encore une fois regarder l'importance de chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "### Arbre de décision\n",
    "# Ramener les importances entre 0 et 1\n",
    "tree_importances = model_tree_best.feature_importances_\n",
    "tree_importances_min = np.min(tree_importances)\n",
    "tree_importances_max = np.max(tree_importances)\n",
    "tree_importances = (tree_importances-tree_importances_min)/(tree_importances_max-tree_importances_min)\n",
    "\n",
    "# Afficher les importances de l'arbre de décision\n",
    "plt.bar(range(num_features), tree_importances,\n",
    "           label=\"Arbre de décision\", width=0.2)\n",
    "\n",
    "### Régression logistique\n",
    "# Ramener les valeurs absolues des coefficients du modèle linéaire entre 0 et 1 \n",
    "logreg_coeffs = np.abs(grid_logreg.best_estimator_.coef_[0])\n",
    "logreg_coeffs_min = np.min(logreg_coeffs)\n",
    "logreg_coeffs_max = np.max(logreg_coeffs)\n",
    "logreg_coeffs = (logreg_coeffs-logreg_coeffs_min)/(logreg_coeffs_max-logreg_coeffs_min)\n",
    "\n",
    "# Afficher les importances de la regression logistique\n",
    "plt.bar((np.arange(num_features)+0.2), logreg_coeffs,\n",
    "           label=\"Régression logistique\", width=0.2)\n",
    "\n",
    "### Forêt aléatoire\n",
    "# Ramener les importances entre 0 et 1\n",
    "rf_importances = model_rf_best.feature_importances_\n",
    "rf_importances_min = np.min(rf_importances)\n",
    "rf_importances_max = np.max(rf_importances)\n",
    "rf_importances = (rf_importances-rf_importances_min)/(rf_importances_max-rf_importances_min)\n",
    "\n",
    "# Afficher les importances de la forêt\n",
    "plt.bar((np.arange(num_features)+0.4),  rf_importances,\n",
    "           label=\"Forêt aléatoire\", width=0.2)\n",
    "\n",
    "### Boosting\n",
    "# Ramener les importances entre 0 et 1\n",
    "boost_importances = model_boost_best.feature_importances_\n",
    "boost_importances_min = np.min(boost_importances)\n",
    "boost_importances_max = np.max(boost_importances)\n",
    "boost_importances = (boost_importances-boost_importances_min)/(boost_importances_max-boost_importances_min)\n",
    "\n",
    "# Afficher les importances du boosting\n",
    "plt.bar((np.arange(num_features)+0.6),  boost_importances,\n",
    "           label=\"Boosting\", width=0.2)\n",
    "\n",
    "# Légende\n",
    "tmp = plt.legend()\n",
    "\n",
    "# Axe des abcisses\n",
    "plt.xlabel('Variables')\n",
    "feature_names = list(df.columns[1:])\n",
    "tmp = plt.xticks(range(num_features), feature_names, \n",
    "                 rotation=90, fontsize=14)\n",
    "\n",
    "# Axe des ordonnées\n",
    "tmp = plt.ylabel('Importance')\n",
    "\n",
    "# Titre\n",
    "tmp = plt.title('Importance des variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "__Question :__ Quelles sont maintenant les variables les plus importantes ? Comment cela se compare-t-il aux modèles précédents ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "## 7. Modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "__Question :__ Lequel de ces modèles choisissez vous comme le plus performant pour classifier les champignons du jeu de test ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "Vous allez maintenant évaluer le modèle que vous avez choisi sur le jeu de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model_rf_best # TODO : insérez ici le nom du modèle que vous avez choisi.\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_pred = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"F1 du modèle choisir sur le jeu de test : %.3f\" % metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "__Question :__ Que pensez-vous de cette performance ? Y-a-t'il un risque de surapprentissage ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "### Matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "Pour mieux interpréter les résultats, on peut aussi visualiser la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Question :__ Que pensez-vous de cette matrice de confusion ? Est-elle satisfaisante ? Rappelez-vous qu'on cherche à prédire si un champignon est comestible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "### Courbe ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "On peut aussi évaluer la performance du modèle __avant seuillage__, c'est-à-dire en utilisant les scores numériques prédits plutôt que les étiquettes binaires, grâce à une [Courbe ROC](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "Les scores avant seuillage d'un modèle de classification de scikit-learn sont accessibles grâce à la méthode `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scores =  my_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_scores)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.plot(fpr, tpr, lw=2)\n",
    "\n",
    "# diagonale\n",
    "plt.plot([0, 1], [0, 1], color='k')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "\n",
    "plt.xlabel('taux de faux positifs')\n",
    "plt.ylabel('taux de vrai positifs')\n",
    "plt.title(\"Courbe ROC du modèle final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "On peut aussi utiliser cette courbe pour déterminer le taux de vrai positifs correspondant à un taux de faux positifs donné, et déterminer le seuil correspondant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fpr = 0.01\n",
    "max_index_where_fpr_acceptable = np.where(fpr <= max_fpr)[0][-1]\n",
    "max_tpr = tpr[max_index_where_fpr_acceptable]\n",
    "print(\"Le taux de vrais positifs correspondant à un taux de faux positifs n'excédant pas %.f %% est de %.f %%\" % ((100*max_fpr), (100*max_tpr)))\n",
    "print(\"Il correspond à un seuil de %.2f sur les prédictions du modèle.\" % thresholds[max_index_where_fpr_acceptable])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
